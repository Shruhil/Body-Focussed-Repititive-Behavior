# -*- coding: utf-8 -*-
"""BFRB_MachileLearningModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJymb75n3LrA7Z5EW_27pVUBU2_J7DPD
"""

# Importing necessary libraries
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib as mpl
import pylab
import numpy as np
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.fft import fft
import scipy.signal as signal
from scipy.signal import welch
from scipy.signal import butter, filtfilt, welch
import pywt
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.impute import SimpleImputer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
import json

!pip install tensorflow

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, BatchNormalization, Dropout,
    Flatten, Dense, Input
)

print("✅ TensorFlow version:", tf.__version__)

# ==============================
# 1️⃣ Import Libraries
# ==============================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

from google.colab import drive
drive.mount('/content/drive')

"""# Raw Signal Visualisation"""

# Load the IMU data
file_path = '/content/nail biting.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size

# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Smoothed Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Smoothed Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/itching on face.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size

# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Smoothed Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Smoothed Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/hair pulling.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size

# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Smoothed Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Smoothed Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/beard pulling.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size

# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Smoothed Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Smoothed Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/nail biting.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# ✅ restrict to region 230-270
imu_data_smoothed_acc_gyro = imu_data_smoothed_acc_gyro.iloc[230:270]

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size
mpl.rcParams['axes.facecolor'] = 'white'   # plot area background
mpl.rcParams['figure.facecolor'] = 'white' # whole figure background


# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()
plt.savefig("imu_signals_region_50_250.png", dpi=400, facecolor='white')

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/itching on face.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# ✅ restrict to region 230–270
imu_data_smoothed_acc_gyro = imu_data_smoothed_acc_gyro.iloc[230:270]

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size
mpl.rcParams['axes.facecolor'] = 'white'   # plot area background
mpl.rcParams['figure.facecolor'] = 'white' # whole figure background


# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()
plt.savefig("imu_signals_region_50_250.png", dpi=400, facecolor='white')

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/hair pulling.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# ✅ restrict to region 230–270
imu_data_smoothed_acc_gyro = imu_data_smoothed_acc_gyro.iloc[230:270]

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size
mpl.rcParams['axes.facecolor'] = 'white'   # plot area background
mpl.rcParams['figure.facecolor'] = 'white' # whole figure background


# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()
plt.savefig("imu_signals_region_50_250.png", dpi=400, facecolor='white')

# Show the plot
plt.show()

# Load the IMU data
file_path = '/content/beard pulling.csv'  # Replace with your actual file path
imu_data = pd.read_csv(file_path)

# Applying a moving average (rolling window) to smooth the accelerometer and gyroscope data (window size of 10)
imu_data_smoothed_acc_gyro = imu_data[['AccelerationX', 'AccelerationY', 'AccelerationZ', 'RotationX', 'RotationY', 'RotationZ']].rolling(window=1, min_periods=1).mean()

# ✅ restrict to region 230–270
imu_data_smoothed_acc_gyro = imu_data_smoothed_acc_gyro.iloc[230:270]

# Update Matplotlib and PyLab settings for a compact, focused visualization
mpl.rcParams['lines.linewidth'] = 2  # Thinner lines
mpl.rcParams['font.weight'] = 'normal'  # Regular font weight
plt.style.use('ggplot')  # Use a minimal Matplotlib style
plt.rc('figure', figsize=(12, 6))  # Smaller figure size
mpl.rcParams['font.family'] = "serif"
pylab.rcParams['ytick.major.pad'] = '10'
pylab.rcParams['xtick.major.pad'] = '10'
mpl.rcParams['axes.labelsize'] = 15  # Smaller label size
mpl.rcParams['axes.linewidth'] = 2
mpl.rcParams['xtick.labelsize'] = 12  # Adjusted for compact view
mpl.rcParams['ytick.labelsize'] = 12
mpl.rcParams['axes.edgecolor'] = 'black'
mpl.rcParams['axes.titlesize'] = 18  # Smaller title size
mpl.rcParams['legend.fontsize'] = 12  # Smaller legend size
mpl.rcParams['axes.facecolor'] = 'white'   # plot area background
mpl.rcParams['figure.facecolor'] = 'white' # whole figure background


# Plotting the smoothed Acceleration and Gyroscope data
plt.figure()

# Plot Accelerometer data
plt.subplot(2, 1, 1)
plt.plot(imu_data_smoothed_acc_gyro['AccelerationX'], label='Acceleration X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationY'], label='Acceleration Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['AccelerationZ'], label='Acceleration Z', color='red')
plt.title('Accelerometer Data')
plt.xlabel('Time (Index)')
plt.ylabel('Acceleration (m/s^2)')
plt.legend()
plt.grid(True)

# Plot Gyroscope data
plt.subplot(2, 1, 2)
plt.plot(imu_data_smoothed_acc_gyro['RotationX'], label='Gyroscope X', color='blue')
plt.plot(imu_data_smoothed_acc_gyro['RotationY'], label='Gyroscope Y', color='black')
plt.plot(imu_data_smoothed_acc_gyro['RotationZ'], label='Gyroscope Z', color='red')
plt.title('Gyroscope Data')
plt.xlabel('Time (Index)')
plt.ylabel('Rotation (rad/s)')
plt.legend()
plt.grid(True)

# Adjust layout for better spacing
plt.tight_layout()
plt.savefig("imu_signals_region_50_250.png", dpi=400, facecolor='white')

# Show the plot
plt.show()

"""#

# Data Preprocessing for nail biting  anayna Mam nail

loading the csv file
"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_nail_biting.csv')
df.head()

"""checking for missing values"""

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

"""converting timestamp to date time format"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

"""Analysing the trend, seasonality and residual of the time series signal"""

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

"""Plotting the pacf for figurring out whether there is a trend or a seasonality in the data"""

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

"""figuring out the sampling frequency of the imu sensor"""

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

"""plotting the power spectral density for analysing noise in the time series signal"""

# Select X-axis columns
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# ==========================
# 2️⃣ Compute PSD using Welch
# ==========================
f_accel, Pxx_accel = welch(accel_x, fs=fs, nperseg=1024)
f_gyro, Pxx_gyro   = welch(gyro_x, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot side-by-side
# ==========================
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# Accelerometer X-axis PSD
axes[0].semilogy(f_accel, Pxx_accel, color='blue')
axes[0].set_title('PSD - AccelerationX')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].grid(True)

# Gyroscope X-axis PSD
axes[1].semilogy(f_gyro, Pxx_gyro, color='red')
axes[1].set_title('PSD - RotationX')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].grid(True)

plt.tight_layout()
plt.show()

"""get the spectral features for qualititatvely analysing the noise in the signal"""

def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    # Normalize PSD for probability interpretation
    Pxx_norm = Pxx / np.sum(Pxx)

    # 1️⃣ Dominant frequency (peak frequency)
    dominant_freq = f[np.argmax(Pxx)]

    # 2️⃣ Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # 3️⃣ Spectral entropy (measure of signal randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # 4️⃣ Band power: energy in specific frequency ranges
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# ⚙️ Compute for both signals
# ==========================
accel_features = spectral_features(f_accel, Pxx_accel)
gyro_features  = spectral_features(f_gyro, Pxx_gyro)
summary_df = pd.DataFrame([accel_features, gyro_features],
                          index=['AccelerationX', 'RotationX'])

print(summary_df)

"""designing a 4th order band pass butterworth filter for removing noise from the data and keeping only meaningful signal"""

# Parameters
lowcut = 0.3   # Hz
highcut = 8.0  # Hz
order = 4      # filter order
fs = fs        # your sampling frequency

# --- Define Butterworth bandpass ---
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    y = filtfilt(b, a, data)  # zero-phase filtering
    return y

# --- Apply to IMU signals ---
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

accel_x_filt = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_x_filt  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

"""comparing raw vs filtered signals for checking whether the noise component has been removed or not?"""

# --- Compare PSD before & after filtering ---
f_raw, Pxx_raw = welch(accel_x, fs=fs, nperseg=1024)
f_filt, Pxx_filt = welch(accel_x_filt, fs=fs, nperseg=1024)

plt.figure(figsize=(8,4))
plt.semilogy(f_raw, Pxx_raw, label='Raw')
plt.semilogy(f_filt, Pxx_filt, label='Filtered')
plt.axvline(lowcut, color='green', linestyle='--', label=f'Lowcut {lowcut}Hz')
plt.axvline(highcut, color='red', linestyle='--', label=f'Highcut {highcut}Hz')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density')
plt.title('AccelerationX PSD: Raw vs Filtered')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""calculatint the spectral features for the denoised signal"""

import numpy as np
from scipy.signal import welch
from scipy.stats import entropy

def compute_spectral_features(signal, fs):
    # Welch PSD
    f, Pxx = welch(signal, fs=fs, nperseg=1024)

    # Normalize PSD for entropy calculation
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency (max power)
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx) / np.sum(Pxx)

    # Spectral entropy
    spec_entropy = entropy(Pxx_norm)

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    bp_0_3 = bandpower(f, Pxx, 0, 3)
    bp_3_6 = bandpower(f, Pxx, 3, 6)
    bp_6_12 = bandpower(f, Pxx, 6, 12)

    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bp_0_3,
        '3-6Hz': bp_3_6,
        '6-12Hz': bp_6_12
    }

# Apply to your filtered signals
features_accel = compute_spectral_features(accel_x_filt, fs)
features_gyro  = compute_spectral_features(gyro_x_filt, fs)

features_df = pd.DataFrame([features_accel, features_gyro],
                           index=['AccelerationX', 'RotationX'])
print(features_df)

"""applying various filtering techniques to see which one gives better results and getting the corresponding plots and qualitative features"""

# ====== Wavelet denoising + feature recompute (ready to run) ======
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt
import pandas as pd
from scipy.signal import butter, filtfilt

# ---------- helper: Butterworth bandpass (reuse if you have it) ----------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, data)

# ---------- wavelet denoise ----------
def wavelet_denoise(x, wavelet='db4', mode='per', level=None, method='universal', threshold_scaling=1.0):
    """
    Wavelet denoising (soft threshold) using pywt.
    - wavelet: wavelet name (db4 is a good default for IMU)
    - mode: signal extension mode
    - level: decomposition level; if None, use pywt.dwt_max_level
    - method: threshold method: 'universal' (VisuShrink) or 'sureshrink'
    - threshold_scaling: multiply computed threshold by this factor (<1 to keep more signal)
    """
    # 1) decompose
    max_level = pywt.dwt_max_level(len(x), pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = max(1, min(max_level, int(np.floor(np.log2(len(x)))) - 1))
    coeffs = pywt.wavedec(x, wavelet, mode=mode, level=level)

    # 2) estimate noise sigma from the finest detail coeffs (median absolute deviation)
    detail_coeffs = coeffs[-1]
    sigma = np.median(np.abs(detail_coeffs)) / 0.6745 + 1e-12

    # 3) compute threshold
    if method == 'universal':
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    elif method == 'sureshrink':
        # pywt provides thresholding methods, but implement simple fallback
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    else:
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))

    uthresh *= threshold_scaling

    # 4) threshold detail coefficients (soft)
    denoised_coeffs = coeffs[:1] + [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]
    # 5) reconstruct
    x_denoised = pywt.waverec(denoised_coeffs, wavelet, mode=mode)
    # ensure same length
    x_denoised = x_denoised[:len(x)]
    return x_denoised

# ---------- spectral features ----------
def compute_spectral_features(signal, fs, nperseg=1024):
    f, Pxx = welch(signal, fs=fs, nperseg=nperseg)
    Pxx_sum = np.trapz(Pxx, f) + 1e-12
    Pxx_norm = Pxx / Pxx_sum
    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    # spectral entropy (base e)
    spec_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))
    # band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])
    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bandpower(f, Pxx, 0, 3),
        '3-6Hz': bandpower(f, Pxx, 3, 6),
        '6-12Hz': bandpower(f, Pxx, 6, 12),
        'f': f, 'Pxx': Pxx  # return for optional plotting
    }

# ---------- Parameters ----------------
lowcut = 0.3
highcut = 8.0
order = 4

wavelet = 'db4'
threshold_scaling = 1.0   # <1 keeps more signal; >1 stronger denoising
method = 'universal'      # threshold method

# ---------- Signals ----------
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# 1) Butterworth bandpass (for comparison)
accel_bp = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_bp  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

# 2) Wavelet denoise (option: denoise raw or bandpass result; below we denoise raw)
accel_wv = wavelet_denoise(accel_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)
gyro_wv  = wavelet_denoise(gyro_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)

# 3) Optionally combine: wavelet -> bandpass (denoise then bandpass)
accel_wv_bp = apply_bandpass_filter(accel_wv, lowcut, highcut, fs, order)
gyro_wv_bp  = apply_bandpass_filter(gyro_wv, lowcut, highcut, fs, order)

# 4) Compute features for all variants
def extract_and_format(signal, name):
    feats = compute_spectral_features(signal, fs)
    # drop f, Pxx from summary; keep for plotting separately
    summary = {k: v for k, v in feats.items() if k not in ('f', 'Pxx')}
    return feats, pd.Series(summary, name=name)

variants = {}
variants['raw_accel'] = extract_and_format(accel_x, 'AccelerationX_raw')
variants['bp_accel']  = extract_and_format(accel_bp, 'AccelerationX_bp')
variants['wv_accel']  = extract_and_format(accel_wv, 'AccelerationX_wv')
variants['wvbp_accel']= extract_and_format(accel_wv_bp, 'AccelerationX_wv_bp')

variants['raw_gyro'] = extract_and_format(gyro_x, 'RotationX_raw')
variants['bp_gyro']  = extract_and_format(gyro_bp, 'RotationX_bp')
variants['wv_gyro']  = extract_and_format(gyro_wv, 'RotationX_wv')
variants['wvbp_gyro']= extract_and_format(gyro_wv_bp, 'RotationX_wv_bp')

# summary table
rows = [variants[k][1] for k in variants]
summary_df = pd.concat(rows, axis=1).T
print(summary_df)

def plot_time_and_psd(raw, bp, wv, wvbp, fs, title_prefix='AccelerationX', variant_key='accel'):
    t = np.arange(len(raw)) / fs
    plt.figure(figsize=(12,5))

    # Time domain
    plt.subplot(1,2,1)
    plt.plot(t, raw, alpha=0.6, label='raw')
    plt.plot(t, bp, alpha=0.8, label='butter_bp')
    plt.plot(t, wvbp, alpha=0.8, label='wavelet->bp')
    plt.xlim(0, min(10, t[-1]))
    plt.xlabel('Time (s)')
    plt.ylabel('Signal')
    plt.title(f'{title_prefix} - Time domain (zoom)')
    plt.legend()
    plt.grid(True)

    # PSD domain
    plt.subplot(1,2,2)
    f_raw, P_raw   = variants[f'raw_{variant_key}'][0]['f'], variants[f'raw_{variant_key}'][0]['Pxx']
    f_bp, P_bp     = variants[f'bp_{variant_key}'][0]['f'],  variants[f'bp_{variant_key}'][0]['Pxx']
    f_wv, P_wv     = variants[f'wv_{variant_key}'][0]['f'],  variants[f'wv_{variant_key}'][0]['Pxx']
    f_wvbp, P_wvbp = variants[f'wvbp_{variant_key}'][0]['f'], variants[f'wvbp_{variant_key}'][0]['Pxx']

    plt.semilogy(f_raw, P_raw, label='raw')
    plt.semilogy(f_bp, P_bp, label='butter_bp')
    plt.semilogy(f_wv, P_wv, label='wavelet')
    plt.semilogy(f_wvbp, P_wvbp, label='wavelet->bp')
    plt.axvline(lowcut, color='green', linestyle='--', label=f'lowcut {lowcut}Hz')
    plt.axvline(highcut, color='red', linestyle='--', label=f'highcut {highcut}Hz')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'{title_prefix} - PSD comparison')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""comparing the results of the techniques used above by looking at the plots"""

plot_time_and_psd(accel_x, accel_bp, accel_wv, accel_wv_bp, fs, title_prefix='AccelerationX')
plot_time_and_psd(gyro_x, gyro_bp, gyro_wv, gyro_wv_bp, fs, title_prefix='RotationX')

print(summary_df[['DominantFreq_Hz', 'SpectralEntropy', 'SpectralCentroid_Hz', '0-3Hz', '3-6Hz', '6-12Hz']])

def compute_noise_index(f, Pxx, signal_band=(0.3, 10), noise_band=(15, np.inf)):
    # Boolean masks for each frequency band
    sig_mask = (f >= signal_band[0]) & (f <= signal_band[1])
    noise_mask = (f >= noise_band[0])

    # Integrate only the selected portions
    signal_power = np.trapezoid(Pxx[sig_mask], f[sig_mask])
    noise_power  = np.trapezoid(Pxx[noise_mask], f[noise_mask])

    return noise_power / (signal_power + 1e-12)

for k, (feats, _) in variants.items():
    ni = compute_noise_index(feats['f'], feats['Pxx'])
    print(f"{k:15s}  Noise Index = {ni:.4e}")

signals = {
    "Raw": accel_x,
    "Bandpass": accel_bp,
    "Wavelet": accel_wv,
    "Wavelet+BP": accel_wv_bp,
}


plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (AccelerationX)")
plt.legend()
plt.grid(True)
plt.show()

signals = {
    "Raw": gyro_x,
    "Bandpass": gyro_bp,
    "Wavelet": gyro_wv,
    "Wavelet+BP": gyro_wv_bp,
}

plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (RotationX)")
plt.legend()
plt.grid(True)
plt.show()

# Accelerometer
accel_wv_y = wavelet_denoise(df['AccelerationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
accel_wv_z = wavelet_denoise(df['AccelerationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

accel_wv_bp_y = apply_bandpass_filter(accel_wv_y, lowcut, highcut, fs, order)
accel_wv_bp_z = apply_bandpass_filter(accel_wv_z, lowcut, highcut, fs, order)

# Gyroscope
gyro_wv_y = wavelet_denoise(df['RotationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
gyro_wv_z = wavelet_denoise(df['RotationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

gyro_wv_bp_y = apply_bandpass_filter(gyro_wv_y, lowcut, highcut, fs, order)
gyro_wv_bp_z = apply_bandpass_filter(gyro_wv_z, lowcut, highcut, fs, order)

"""storing the best denoised signals in the dataframe for further processing"""

df_denoised = pd.DataFrame({
    'AccelerationX': accel_wv_bp,
    'AccelerationY': accel_wv_bp_y,
    'AccelerationZ': accel_wv_bp_z,
    'RotationX': gyro_wv_bp,
    'RotationY': gyro_wv_bp_y,
    'RotationZ': gyro_wv_bp_z
})


# Optional: include timestamp if you have it
# df_denoised['Timestamp'] = df['Timestamp']

# Save to CSV
df_denoised.to_csv("denoised_signal_nail_biting_Ananya.csv", index=False)

print("✅ Wavelet + bandpass denoised dataframe stored successfully as CSV.")

df_denoised.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_denoised[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_denoised[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

"""#Data preprocessing for hair pulling ananya"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_hair_pulling.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.3
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.3, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_hair_Ananya.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for face Itching anaya

"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_face_scratching.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf

# List of signal columns
acc_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes = ['RotationX', 'RotationY', 'RotationZ']

# ---- Accelerometer PACF ----
for axis in acc_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# ---- Gyroscope PACF ----
for axis in gyro_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.1
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.1, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_face_itching_Ananya.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for nail biting  person 2

loading the csv file
"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person2.csv')
df.head()

"""checking for missing values"""

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

"""converting timestamp to date time format"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

"""Analysing the trend, seasonality and residual of the time series signal"""

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

"""Plotting the pacf for figurring out whether there is a trend or a seasonality in the data"""

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

"""figuring out the sampling frequency of the imu sensor"""

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

"""plotting the power spectral density for analysing noise in the time series signal"""

# Select X-axis columns
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# ==========================
# 2️⃣ Compute PSD using Welch
# ==========================
f_accel, Pxx_accel = welch(accel_x, fs=fs, nperseg=1024)
f_gyro, Pxx_gyro   = welch(gyro_x, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot side-by-side
# ==========================
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# Accelerometer X-axis PSD
axes[0].semilogy(f_accel, Pxx_accel, color='blue')
axes[0].set_title('PSD - AccelerationX')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].grid(True)

# Gyroscope X-axis PSD
axes[1].semilogy(f_gyro, Pxx_gyro, color='red')
axes[1].set_title('PSD - RotationX')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].grid(True)

plt.tight_layout()
plt.show()

"""get the spectral features for qualititatvely analysing the noise in the signal"""

def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    # Normalize PSD for probability interpretation
    Pxx_norm = Pxx / np.sum(Pxx)

    # 1️⃣ Dominant frequency (peak frequency)
    dominant_freq = f[np.argmax(Pxx)]

    # 2️⃣ Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # 3️⃣ Spectral entropy (measure of signal randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # 4️⃣ Band power: energy in specific frequency ranges
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# ⚙️ Compute for both signals
# ==========================
accel_features = spectral_features(f_accel, Pxx_accel)
gyro_features  = spectral_features(f_gyro, Pxx_gyro)
summary_df = pd.DataFrame([accel_features, gyro_features],
                          index=['AccelerationX', 'RotationX'])

print(summary_df)

"""designing a 4th order band pass butterworth filter for removing noise from the data and keeping only meaningful signal"""

# Parameters
lowcut = 0.3   # Hz
highcut = 8.0  # Hz
order = 4      # filter order
fs = fs        # your sampling frequency

# --- Define Butterworth bandpass ---
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    y = filtfilt(b, a, data)  # zero-phase filtering
    return y

# --- Apply to IMU signals ---
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

accel_x_filt = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_x_filt  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

"""comparing raw vs filtered signals for checking whether the noise component has been removed or not?"""

# --- Compare PSD before & after filtering ---
f_raw, Pxx_raw = welch(accel_x, fs=fs, nperseg=1024)
f_filt, Pxx_filt = welch(accel_x_filt, fs=fs, nperseg=1024)

plt.figure(figsize=(8,4))
plt.semilogy(f_raw, Pxx_raw, label='Raw')
plt.semilogy(f_filt, Pxx_filt, label='Filtered')
plt.axvline(lowcut, color='green', linestyle='--', label=f'Lowcut {lowcut}Hz')
plt.axvline(highcut, color='red', linestyle='--', label=f'Highcut {highcut}Hz')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density')
plt.title('AccelerationX PSD: Raw vs Filtered')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""calculatint the spectral features for the denoised signal"""

import numpy as np
from scipy.signal import welch
from scipy.stats import entropy

def compute_spectral_features(signal, fs):
    # Welch PSD
    f, Pxx = welch(signal, fs=fs, nperseg=1024)

    # Normalize PSD for entropy calculation
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency (max power)
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx) / np.sum(Pxx)

    # Spectral entropy
    spec_entropy = entropy(Pxx_norm)

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    bp_0_3 = bandpower(f, Pxx, 0, 3)
    bp_3_6 = bandpower(f, Pxx, 3, 6)
    bp_6_12 = bandpower(f, Pxx, 6, 12)

    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bp_0_3,
        '3-6Hz': bp_3_6,
        '6-12Hz': bp_6_12
    }

# Apply to your filtered signals
features_accel = compute_spectral_features(accel_x_filt, fs)
features_gyro  = compute_spectral_features(gyro_x_filt, fs)

features_df = pd.DataFrame([features_accel, features_gyro],
                           index=['AccelerationX', 'RotationX'])
print(features_df)

"""applying various filtering techniques to see which one gives better results and getting the corresponding plots and qualitative features"""

# ====== Wavelet denoising + feature recompute (ready to run) ======
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt
import pandas as pd
from scipy.signal import butter, filtfilt

# ---------- helper: Butterworth bandpass (reuse if you have it) ----------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, data)

# ---------- wavelet denoise ----------
def wavelet_denoise(x, wavelet='db4', mode='per', level=None, method='universal', threshold_scaling=1.0):
    """
    Wavelet denoising (soft threshold) using pywt.
    - wavelet: wavelet name (db4 is a good default for IMU)
    - mode: signal extension mode
    - level: decomposition level; if None, use pywt.dwt_max_level
    - method: threshold method: 'universal' (VisuShrink) or 'sureshrink'
    - threshold_scaling: multiply computed threshold by this factor (<1 to keep more signal)
    """
    # 1) decompose
    max_level = pywt.dwt_max_level(len(x), pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = max(1, min(max_level, int(np.floor(np.log2(len(x)))) - 1))
    coeffs = pywt.wavedec(x, wavelet, mode=mode, level=level)

    # 2) estimate noise sigma from the finest detail coeffs (median absolute deviation)
    detail_coeffs = coeffs[-1]
    sigma = np.median(np.abs(detail_coeffs)) / 0.6745 + 1e-12

    # 3) compute threshold
    if method == 'universal':
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    elif method == 'sureshrink':
        # pywt provides thresholding methods, but implement simple fallback
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    else:
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))

    uthresh *= threshold_scaling

    # 4) threshold detail coefficients (soft)
    denoised_coeffs = coeffs[:1] + [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]
    # 5) reconstruct
    x_denoised = pywt.waverec(denoised_coeffs, wavelet, mode=mode)
    # ensure same length
    x_denoised = x_denoised[:len(x)]
    return x_denoised

# ---------- spectral features ----------
def compute_spectral_features(signal, fs, nperseg=1024):
    f, Pxx = welch(signal, fs=fs, nperseg=nperseg)
    Pxx_sum = np.trapz(Pxx, f) + 1e-12
    Pxx_norm = Pxx / Pxx_sum
    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    # spectral entropy (base e)
    spec_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))
    # band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])
    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bandpower(f, Pxx, 0, 3),
        '3-6Hz': bandpower(f, Pxx, 3, 6),
        '6-12Hz': bandpower(f, Pxx, 6, 12),
        'f': f, 'Pxx': Pxx  # return for optional plotting
    }

# ---------- Parameters ----------------
lowcut = 0.3
highcut = 8.0
order = 4

wavelet = 'db4'
threshold_scaling = 1.0   # <1 keeps more signal; >1 stronger denoising
method = 'universal'      # threshold method

# ---------- Signals ----------
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# 1) Butterworth bandpass (for comparison)
accel_bp = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_bp  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

# 2) Wavelet denoise (option: denoise raw or bandpass result; below we denoise raw)
accel_wv = wavelet_denoise(accel_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)
gyro_wv  = wavelet_denoise(gyro_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)

# 3) Optionally combine: wavelet -> bandpass (denoise then bandpass)
accel_wv_bp = apply_bandpass_filter(accel_wv, lowcut, highcut, fs, order)
gyro_wv_bp  = apply_bandpass_filter(gyro_wv, lowcut, highcut, fs, order)

# 4) Compute features for all variants
def extract_and_format(signal, name):
    feats = compute_spectral_features(signal, fs)
    # drop f, Pxx from summary; keep for plotting separately
    summary = {k: v for k, v in feats.items() if k not in ('f', 'Pxx')}
    return feats, pd.Series(summary, name=name)

variants = {}
variants['raw_accel'] = extract_and_format(accel_x, 'AccelerationX_raw')
variants['bp_accel']  = extract_and_format(accel_bp, 'AccelerationX_bp')
variants['wv_accel']  = extract_and_format(accel_wv, 'AccelerationX_wv')
variants['wvbp_accel']= extract_and_format(accel_wv_bp, 'AccelerationX_wv_bp')

variants['raw_gyro'] = extract_and_format(gyro_x, 'RotationX_raw')
variants['bp_gyro']  = extract_and_format(gyro_bp, 'RotationX_bp')
variants['wv_gyro']  = extract_and_format(gyro_wv, 'RotationX_wv')
variants['wvbp_gyro']= extract_and_format(gyro_wv_bp, 'RotationX_wv_bp')

# summary table
rows = [variants[k][1] for k in variants]
summary_df = pd.concat(rows, axis=1).T
print(summary_df)

def plot_time_and_psd(raw, bp, wv, wvbp, fs, title_prefix='AccelerationX', variant_key='accel'):
    t = np.arange(len(raw)) / fs
    plt.figure(figsize=(12,5))

    # Time domain
    plt.subplot(1,2,1)
    plt.plot(t, raw, alpha=0.6, label='raw')
    plt.plot(t, bp, alpha=0.8, label='butter_bp')
    plt.plot(t, wvbp, alpha=0.8, label='wavelet->bp')
    plt.xlim(0, min(10, t[-1]))
    plt.xlabel('Time (s)')
    plt.ylabel('Signal')
    plt.title(f'{title_prefix} - Time domain (zoom)')
    plt.legend()
    plt.grid(True)

    # PSD domain
    plt.subplot(1,2,2)
    f_raw, P_raw   = variants[f'raw_{variant_key}'][0]['f'], variants[f'raw_{variant_key}'][0]['Pxx']
    f_bp, P_bp     = variants[f'bp_{variant_key}'][0]['f'],  variants[f'bp_{variant_key}'][0]['Pxx']
    f_wv, P_wv     = variants[f'wv_{variant_key}'][0]['f'],  variants[f'wv_{variant_key}'][0]['Pxx']
    f_wvbp, P_wvbp = variants[f'wvbp_{variant_key}'][0]['f'], variants[f'wvbp_{variant_key}'][0]['Pxx']

    plt.semilogy(f_raw, P_raw, label='raw')
    plt.semilogy(f_bp, P_bp, label='butter_bp')
    plt.semilogy(f_wv, P_wv, label='wavelet')
    plt.semilogy(f_wvbp, P_wvbp, label='wavelet->bp')
    plt.axvline(lowcut, color='green', linestyle='--', label=f'lowcut {lowcut}Hz')
    plt.axvline(highcut, color='red', linestyle='--', label=f'highcut {highcut}Hz')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'{title_prefix} - PSD comparison')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""comparing the results of the techniques used above by looking at the plots"""

plot_time_and_psd(accel_x, accel_bp, accel_wv, accel_wv_bp, fs, title_prefix='AccelerationX')
plot_time_and_psd(gyro_x, gyro_bp, gyro_wv, gyro_wv_bp, fs, title_prefix='RotationX')

print(summary_df[['DominantFreq_Hz', 'SpectralEntropy', 'SpectralCentroid_Hz', '0-3Hz', '3-6Hz', '6-12Hz']])

def compute_noise_index(f, Pxx, signal_band=(0.3, 10), noise_band=(15, np.inf)):
    # Boolean masks for each frequency band
    sig_mask = (f >= signal_band[0]) & (f <= signal_band[1])
    noise_mask = (f >= noise_band[0])

    # Integrate only the selected portions
    signal_power = np.trapezoid(Pxx[sig_mask], f[sig_mask])
    noise_power  = np.trapezoid(Pxx[noise_mask], f[noise_mask])

    return noise_power / (signal_power + 1e-12)

for k, (feats, _) in variants.items():
    ni = compute_noise_index(feats['f'], feats['Pxx'])
    print(f"{k:15s}  Noise Index = {ni:.4e}")

signals = {
    "Raw": accel_x,
    "Bandpass": accel_bp,
    "Wavelet": accel_wv,
    "Wavelet+BP": accel_wv_bp,
}


plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (AccelerationX)")
plt.legend()
plt.grid(True)
plt.show()

signals = {
    "Raw": gyro_x,
    "Bandpass": gyro_bp,
    "Wavelet": gyro_wv,
    "Wavelet+BP": gyro_wv_bp,
}

plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (RotationX)")
plt.legend()
plt.grid(True)
plt.show()

# Accelerometer
accel_wv_y = wavelet_denoise(df['AccelerationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
accel_wv_z = wavelet_denoise(df['AccelerationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

accel_wv_bp_y = apply_bandpass_filter(accel_wv_y, lowcut, highcut, fs, order)
accel_wv_bp_z = apply_bandpass_filter(accel_wv_z, lowcut, highcut, fs, order)

# Gyroscope
gyro_wv_y = wavelet_denoise(df['RotationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
gyro_wv_z = wavelet_denoise(df['RotationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

gyro_wv_bp_y = apply_bandpass_filter(gyro_wv_y, lowcut, highcut, fs, order)
gyro_wv_bp_z = apply_bandpass_filter(gyro_wv_z, lowcut, highcut, fs, order)

"""storing the best denoised signals in the dataframe for further processing"""

df_denoised = pd.DataFrame({
    'AccelerationX': accel_wv_bp,
    'AccelerationY': accel_wv_bp_y,
    'AccelerationZ': accel_wv_bp_z,
    'RotationX': gyro_wv_bp,
    'RotationY': gyro_wv_bp_y,
    'RotationZ': gyro_wv_bp_z
})


# Optional: include timestamp if you have it
# df_denoised['Timestamp'] = df['Timestamp']

# Save to CSV
df_denoised.to_csv("denoised_signal_nail_biting_Person2.csv", index=False)

print("✅ Wavelet + bandpass denoised dataframe stored successfully as CSV.")

df_denoised.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_denoised[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_denoised[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

"""#Data preprocessing for hair pulling person2"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person2.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.3
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.3, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_hair_Person2.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for face Itching person2

"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Face_scratching_person1.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf

# List of signal columns
acc_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes = ['RotationX', 'RotationY', 'RotationZ']

# ---- Accelerometer PACF ----
for axis in acc_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# ---- Gyroscope PACF ----
for axis in gyro_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.1
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.1, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_face_itching_Person2.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for beard pulling person 2

"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person2.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1

    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [
        pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]
    ]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')

    # Adjust length if needed
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised


# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a


def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, x)


# -------------------------------
# Combined Wavelet + Bandpass
# -------------------------------
def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f


# -------------------------------
# Band Power Fraction Helper
# -------------------------------
def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapezoid(Pxx, f)
    band = np.trapezoid(Pxx[idx], f[idx])
    return band / (total + 1e-20)


# -------------------------------
# Parameters
# -------------------------------
fs = 50  # sampling rate

# Optimized bandpass per axis (from your spectral data)
filter_params = {
    'AccelerationX': (0.3, 10.0),
    'AccelerationY': (0.3, 10.0),
    'AccelerationZ': (0.3, 12.0),
    'RotationX':     (0.3, 6.0),
    'RotationY':     (0.3, 6.0),
    'RotationZ':     (0.3, 6.0),
}

axes_all = list(filter_params.keys())

# -------------------------------
# Apply to all signals
# -------------------------------
results = {}
for col in axes_all:
    lowcut, highcut = filter_params[col]
    sig = df[col].values

    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

    # --- PSD and passband fraction ---
    f_raw, P_raw = welch(sig, fs=fs, nperseg=1024)
    f_den, P_den = welch(deno, fs=fs, nperseg=1024)
    f_fil, P_fil = welch(filtered, fs=fs, nperseg=1024)

    frac_raw = band_power_fraction(f_raw, P_raw, lowcut, highcut)
    frac_fil = band_power_fraction(f_fil, P_fil, lowcut, highcut)

    print(f"{col}:  low={lowcut:.1f}Hz  high={highcut:.1f}Hz  |  "
          f"raw_frac={frac_raw:.3f}  filtered_frac={frac_fil:.3f}")

    # --- PSD plot ---
    plt.figure(figsize=(8, 4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.6)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.6)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col == 'AccelerationX' else "")
    plt.axvline(highcut, color='red', linestyle='--', label='Highcut' if col == 'AccelerationX' else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import numpy as np
from scipy.signal import welch

# ------------------------------------
# Band Power Fraction (updated)
# ------------------------------------
def band_power_fraction(f, Pxx, low, high):
    """Compute fraction of signal power within a given frequency band."""
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapezoid(Pxx, f)
    band = np.trapezoid(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # Avoid division by zero


# ------------------------------------
# Frequency bands per axis
# (Based on earlier spectral feature table)
# ------------------------------------
filter_params = {
    'AccelerationX': (0.3, 10.0),
    'AccelerationY': (0.3, 10.0),
    'AccelerationZ': (0.3, 12.0),
    'RotationX':     (0.3, 6.0),
    'RotationY':     (0.3, 6.0),
    'RotationZ':     (0.3, 6.0),
}

axes_all = list(filter_params.keys())

# ------------------------------------
# Compute passband fraction for all axes
# ------------------------------------
passband_results = {}

for col in axes_all:
    lowcut, highcut = filter_params[col]

    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'lowcut': lowcut,
        'highcut': highcut,
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# ------------------------------------
# Print results neatly
# ------------------------------------
print("Passband Power Fractions by Axis\n" + "-" * 45)
for col in axes_all:
    vals = passband_results[col]
    print(f"{col}:  [{vals['lowcut']} - {vals['highcut']} Hz]")
    print(f"  raw passband frac:      {vals['raw']:.6f}")
    print(f"  denoised passband frac: {vals['denoised']:.6f}")
    print(f"  filtered passband frac: {vals['filtered']:.6f}\n")

df_raw = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person2.csv")

df_raw.head()

# Define the IMU signal column names
axes_accel = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
axes_gyro  = ['RotationX', 'RotationY', 'RotationZ']

# --- 1️⃣ Create new DataFrame with timestamps ---
df_final = pd.DataFrame()
df_final['Timestamp'] = df_raw['Timestamp']  # preserve original timestamps

# --- 2️⃣ Add final filtered accelerometer and gyroscope data ---
for col in axes_accel + axes_gyro:
    df_final[col] = results[col]['filtered']

# --- 3️⃣ Save to CSV ---
output_file = "beard_pulling_filtered_Person2.csv"
df_final.to_csv(output_file, index=False)

print(f"✅ Final filtered IMU data saved to {output_file}")

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for nail biting  person 1

loading the csv file
"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person1.csv')
print("Dataset shape:", df.shape)
print(df.head())

"""checking for missing values"""

missing_values = df.isnull().sum()
print("\nMissing values per column:\n", missing_values)

"""converting timestamp to date time format"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

print("\nDataFrame with timestamp index:")
print(df.head())

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']
imu_data = df[imu_cols]

"""Analysing the trend, seasonality and residual of the time series signal"""

dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"\nEstimated Sampling Rate: {fs:.2f} Hz")

n_obs = len(imu_data)
max_period = n_obs // 2  # Need at least 2 complete cycles
safe_period = min(16, max_period)  # Use smaller value for safety

print(f"\nDataset has {n_obs} observations")
print(f"Maximum period for seasonal decomposition: {max_period}")
print(f"Using period: {safe_period}")

"""Plotting the pacf for figurring out whether there is a trend or a seasonality in the data"""

if n_obs >= 2 * safe_period:
    print(f"\n✓ Performing seasonal decomposition with period={safe_period}")
    result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=safe_period)
    result.plot()
    plt.suptitle(f"Decomposition of AccelerationX (period={safe_period})", fontsize=14)
    plt.tight_layout()
    plt.show()

    # Decompose all IMU columns
    for col in imu_cols:
        result = seasonal_decompose(imu_data[col], model='additive', period=safe_period)
        result.plot()
        plt.suptitle(f"Decomposition of {col} (period={safe_period})", fontsize=14)
        plt.tight_layout()
        plt.show()
else:
    print(f"\n⚠ Skipping seasonal decomposition: need at least {2*safe_period} observations, have {n_obs}")
    print("Using alternative visualization: Moving Average")

    # Alternative: Show moving average
    window = min(5, n_obs // 3)
    plt.figure(figsize=(12, 4))
    plt.plot(imu_data['AccelerationX'], label='Original', alpha=0.7)
    plt.plot(imu_data['AccelerationX'].rolling(window=window).mean(),
             label=f'Moving Average (window={window})', linewidth=2)
    plt.title('AccelerationX with Moving Average')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

"""figuring out the sampling frequency of the imu sensor"""

max_lags = min(n_obs // 2 - 1, 20)  # Safer lag limit
if max_lags >= 1:
    plt.figure(figsize=(8, 4))
    plot_pacf(imu_data['AccelerationX'], lags=max_lags, method='ywm')
    plt.title(f"Partial Autocorrelation - AccelerationX (lags={max_lags})")
    plt.show()
else:
    print(f"⚠ Skipping PACF: need more observations (have {n_obs})")

# ================== POWER SPECTRAL DENSITY ==================
accel_x = df['AccelerationX'].values
gyro_x = df['RotationX'].values

# Adjust nperseg for small datasets
nperseg = min(256, len(accel_x))  # Use smaller window for short signals

f_accel, Pxx_accel = welch(accel_x, fs=fs, nperseg=nperseg)
f_gyro, Pxx_gyro = welch(gyro_x, fs=fs, nperseg=nperseg)

fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

axes[0].semilogy(f_accel, Pxx_accel, color='blue')
axes[0].set_title('PSD - AccelerationX')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].grid(True)

axes[1].semilogy(f_gyro, Pxx_gyro, color='red')
axes[1].set_title('PSD - RotationX')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].grid(True)

plt.tight_layout()
plt.show()

"""plotting the power spectral density for analysing noise in the time series signal"""

def spectral_features(f, Pxx, band_limits=[(0, 3), (3, 6), (6, 12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx_norm)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

accel_features = spectral_features(f_accel, Pxx_accel)
gyro_features = spectral_features(f_gyro, Pxx_gyro)
summary_df = pd.DataFrame([accel_features, gyro_features],
                          index=['AccelerationX', 'RotationX'])
print("\nSpectral Features (Raw Signals):")
print(summary_df)

"""get the spectral features for qualititatvely analysing the noise in the signal"""

lowcut = 0.3
highcut = 8.0
order = 4
wavelet = 'db4'

def butter_bandpass(lowcut, highcut, fs, order=4):
    """Design Butterworth bandpass filter."""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    """Apply Butterworth bandpass filter."""
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    y = filtfilt(b, a, data)
    return y

def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    """
    Wavelet denoising using soft thresholding.

    Parameters:
    -----------
    x : array_like
        Input signal to denoise
    wavelet : str
        Wavelet name (default: 'db4')
    level : int or None
        Decomposition level (default: auto-calculated)
    mode : str
        Thresholding mode: 'soft' or 'hard' (default: 'soft')

    Returns:
    --------
    x_denoised : ndarray
        Denoised signal
    """
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)

    # Auto-calculate level if not provided
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1

    # Decompose signal
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')

    # Estimate noise level from finest detail coefficients
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745

    # Universal threshold
    uthresh = sigma * np.sqrt(2 * np.log(n))

    # Threshold all detail coefficients
    coeffs_thresh = [coeffs[0]] + [
        pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]
    ]

    # Reconstruct signal
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')

    # Adjust length if needed
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')

    return x_denoised

"""designing a 4th order band pass butterworth filter for removing noise from the data and keeping only meaningful signal"""

print("\n🔧 Applying Filters...")

# Bandpass only
accel_bp = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_bp = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

# Wavelet only
accel_wv = wavelet_denoise(accel_x, wavelet=wavelet)
gyro_wv = wavelet_denoise(gyro_x, wavelet=wavelet)

# Wavelet + Bandpass (Best approach)
accel_wv_bp = apply_bandpass_filter(accel_wv, lowcut, highcut, fs, order)
gyro_wv_bp = apply_bandpass_filter(gyro_wv, lowcut, highcut, fs, order)

print("✅ Filters applied successfully")

f_raw, Pxx_raw = welch(accel_x, fs=fs, nperseg=nperseg)
f_filt, Pxx_filt = welch(accel_wv_bp, fs=fs, nperseg=nperseg)

plt.figure(figsize=(10, 5))
plt.semilogy(f_raw, Pxx_raw, label='Raw', alpha=0.7)
plt.semilogy(f_filt, Pxx_filt, label='Wavelet+Bandpass', linewidth=2)
plt.axvline(lowcut, color='green', linestyle='--', label=f'Lowcut {lowcut}Hz')
plt.axvline(highcut, color='red', linestyle='--', label=f'Highcut {highcut}Hz')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density')
plt.title('AccelerationX PSD: Raw vs Filtered')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""comparing raw vs filtered signals for checking whether the noise component has been removed or not?"""

def compute_spectral_features(signal, fs, nperseg=None):
    """Compute spectral features from signal."""
    if nperseg is None:
        nperseg = min(256, len(signal))

    f, Pxx = welch(signal, fs=fs, nperseg=nperseg)
    Pxx_sum = np.trapz(Pxx, f) + 1e-12
    Pxx_norm = Pxx / Pxx_sum

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spec_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bandpower(f, Pxx, 0, 3),
        '3-6Hz': bandpower(f, Pxx, 3, 6),
        '6-12Hz': bandpower(f, Pxx, 6, 12),
        'f': f, 'Pxx': Pxx
    }

features_accel_filt = compute_spectral_features(accel_wv_bp, fs)
features_gyro_filt = compute_spectral_features(gyro_wv_bp, fs)

features_df = pd.DataFrame([
    {k: v for k, v in features_accel_filt.items() if k not in ('f', 'Pxx')},
    {k: v for k, v in features_gyro_filt.items() if k not in ('f', 'Pxx')}
], index=['AccelerationX_filtered', 'RotationX_filtered'])

print("\nSpectral Features (Filtered Signals):")
print(features_df)

"""calculatint the spectral features for the denoised signal"""

t = np.arange(len(accel_x)) / fs

signals = {
    "Raw": accel_x,
    "Bandpass": accel_bp,
    "Wavelet": accel_wv,
    "Wavelet+BP": accel_wv_bp,
}

plt.figure(figsize=(14, 6))
for name, sig in signals.items():
    plt.plot(t, sig, label=name, alpha=0.7)
plt.xlabel('Time (s)')
plt.ylabel('Acceleration (m/s²)')
plt.title("Comparison of Filtered Signals (AccelerationX)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

zoom_samples = min(1000, len(accel_x))
plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:zoom_samples], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals - Zoomed (AccelerationX)")
plt.xlabel("Sample Index")
plt.ylabel("Acceleration (m/s²)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""applying various filtering techniques to see which one gives better results and getting the corresponding plots and qualitative features"""

signals_gyro = {
    "Raw": gyro_x,
    "Bandpass": gyro_bp,
    "Wavelet": gyro_wv,
    "Wavelet+BP": gyro_wv_bp,
}

plt.figure(figsize=(12, 6))
for name, sig in signals_gyro.items():
    plt.plot(sig[:zoom_samples], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals - Zoomed (RotationX)")
plt.xlabel("Sample Index")
plt.ylabel("Angular Velocity (rad/s)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""comparing the results of the techniques used above by looking at the plots"""

print("\n🔧 Processing all axes...")

# Accelerometer
accel_wv_y = wavelet_denoise(df['AccelerationY'].values, wavelet=wavelet)
accel_wv_z = wavelet_denoise(df['AccelerationZ'].values, wavelet=wavelet)

accel_wv_bp_y = apply_bandpass_filter(accel_wv_y, lowcut, highcut, fs, order)
accel_wv_bp_z = apply_bandpass_filter(accel_wv_z, lowcut, highcut, fs, order)

# Gyroscope
gyro_wv_y = wavelet_denoise(df['RotationY'].values, wavelet=wavelet)
gyro_wv_z = wavelet_denoise(df['RotationZ'].values, wavelet=wavelet)

gyro_wv_bp_y = apply_bandpass_filter(gyro_wv_y, lowcut, highcut, fs, order)
gyro_wv_bp_z = apply_bandpass_filter(gyro_wv_z, lowcut, highcut, fs, order)

print("✅ All axes processed")

# ================== SAVE DENOISED DATA ==================
df_denoised = pd.DataFrame({
    'AccelerationX': accel_wv_bp,
    'AccelerationY': accel_wv_bp_y,
    'AccelerationZ': accel_wv_bp_z,
    'RotationX': gyro_wv_bp,
    'RotationY': gyro_wv_bp_y,
    'RotationZ': gyro_wv_bp_z
})

df_denoised.to_csv("denoised_signal_nail_biting_Person1.csv", index=False)
print("\n✅ Wavelet + bandpass denoised dataframe saved successfully")
print(f"   Output file: denoised_signal_nail_biting_Person1.csv")
print(f"   Shape: {df_denoised.shape}")
print("\nFirst few rows of denoised data:")
print(df_denoised.head())

raw_acc = df[['AccelerationX', 'AccelerationY', 'AccelerationZ']].values
raw_gyro = df[['RotationX', 'RotationY', 'RotationZ']].values

denoised_acc = df_denoised[['AccelerationX', 'AccelerationY', 'AccelerationZ']].values
denoised_gyro = df_denoised[['RotationX', 'RotationY', 'RotationZ']].values

axes_labels = ['X', 'Y', 'Z']

# Accelerometer comparison
print("\n📊 Visualizing Accelerometer Signals...")
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12, 4))
    plt.plot(t, raw_acc[:, i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:, i], alpha=0.9, linewidth=2, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

#Gyroscope comparison
print("\n📊 Visualizing Gyroscope Signals...")
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12, 4))
    plt.plot(t, raw_gyro[:, i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:, i], alpha=0.9, linewidth=2, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def calculate_noise_rms(raw, denoised):
    """Calculate RMS of removed noise."""
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

def calculate_snr_improvement(raw, denoised):
    """Calculate SNR improvement in dB."""
    noise_after = raw - denoised
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / (np.var(noise_after, axis=0) + 1e-12))
    return snr_improvement

print("\n📈 Noise Analysis:")
print("=" * 60)

acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

print("\nAccelerometer:")
for i, axis in enumerate(axes_labels):
    print(f"  {axis}-Axis:")
    print(f"    Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"    SNR Improvement: {acc_snr_improvement[i]:.2f} dB")

gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

print("\nGyroscope:")
for i, axis in enumerate(axes_labels):
    print(f"  {axis}-Axis:")
    print(f"    Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"    SNR Improvement: {gyro_snr_improvement[i]:.2f} dB")

print("\n" + "=" * 60)
print("✅ Preprocessing Complete!")
print("=" * 60)

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_denoised[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_denoised[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

"""#Data preprocessing for hair pulling person1"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person1.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.3
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.3, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_hair_Person1.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for face Itching person1

"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Face_Scratching_person2.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf

# List of signal columns
acc_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes = ['RotationX', 'RotationY', 'RotationZ']

# ---- Accelerometer PACF ----
for axis in acc_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# ---- Gyroscope PACF ----
for axis in gyro_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.1
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.1, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_face_itching_Person1.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for beard pulling person 1

"""

df = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person1.csv')
print("Dataset shape:", df.shape)
print(df.head())

missing_values = df.isnull().sum()
print("\nMissing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']
imu_data = df[imu_cols]

dt = np.diff(df.index.values.astype(np.int64) / 1e9)
fs = 1.0 / np.mean(dt)
print(f"\nEstimated Sampling Rate: {fs:.2f} Hz")

n_obs = len(imu_data)
max_period = n_obs // 2
safe_period = min(50, max_period)

print(f"\nDataset has {n_obs} observations")
print(f"Using period: {safe_period}")

if n_obs >= 2 * safe_period:
    print(f"✓ Performing seasonal decomposition with period={safe_period}")
    result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=safe_period)
    result.plot()
    plt.suptitle(f"Decomposition of AccelerationX (period={safe_period})", fontsize=14)
    plt.tight_layout()
    plt.show()

for col in imu_cols:
        result = seasonal_decompose(imu_data[col], model='additive', period=safe_period)
        result.plot()
        plt.suptitle(f"Decomposition of {col}", fontsize=14)
        plt.tight_layout()
        plt.show()
else:
    print(f"⚠ Skipping seasonal decomposition: insufficient data")

accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes = ['RotationX', 'RotationY', 'RotationZ']

# Adjust nperseg for dataset size
nperseg = min(1024, len(df))

# Extract signals
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values
gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# Compute PSD
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=nperseg)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=nperseg)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=nperseg)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=nperseg)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=nperseg)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=nperseg)

fig, axes = plt.subplots(2, 1, figsize=(12, 10))

axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

def spectral_features(f, Pxx, band_limits=[(0, 3), (3, 6), (6, 12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx_norm)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

features = {}
for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=nperseg)
    features[col] = spectral_features(f, Pxx)

summary_df = pd.DataFrame(features).T
print("\nSpectral Features (Raw Signals):")
print(summary_df)

def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    """Wavelet denoising using soft thresholding."""
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1

    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))

    coeffs_thresh = [coeffs[0]] + [
        pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]
    ]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')

    # Adjust length
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')

    return x_denoised

def butter_bandpass(lowcut, highcut, fs, order=4):
    """Design Butterworth bandpass filter."""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    """Apply Butterworth bandpass filter."""
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    """Combined wavelet denoising and bandpass filtering."""
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

def band_power_fraction(f, Pxx, low, high):
    """Compute fraction of signal power within frequency band."""
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)

# ================== OPTIMIZED FILTER PARAMETERS ==================
filter_params = {
    'AccelerationX': (0.3, 10.0),
    'AccelerationY': (0.3, 10.0),
    'AccelerationZ': (0.3, 12.0),
    'RotationX':     (0.3, 6.0),
    'RotationY':     (0.3, 6.0),
    'RotationZ':     (0.3, 6.0),
}

axes_all = list(filter_params.keys())

results = {}
print("\nApplying Wavelet + Bandpass Filtering:")
print("-" * 60)

for col in axes_all:
    lowcut, highcut = filter_params[col]
    sig = df[col].values

    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

    # Compute PSD
    f_raw, P_raw = welch(sig, fs=fs, nperseg=nperseg)
    f_den, P_den = welch(deno, fs=fs, nperseg=nperseg)
    f_fil, P_fil = welch(filtered, fs=fs, nperseg=nperseg)

    frac_raw = band_power_fraction(f_raw, P_raw, lowcut, highcut)
    frac_fil = band_power_fraction(f_fil, P_fil, lowcut, highcut)

    print(f"{col}:  [{lowcut:.1f}-{highcut:.1f}Hz]  |  "
          f"raw_frac={frac_raw:.3f}  filtered_frac={frac_fil:.3f}")

    # Plot PSD comparison
    plt.figure(figsize=(10, 5))
    plt.semilogy(f_raw, P_raw, label='Raw', alpha=0.6)
    plt.semilogy(f_den, P_den, label='Wavelet Denoised', alpha=0.6)
    plt.semilogy(f_fil, P_fil, label='Denoise + Bandpass', alpha=0.9, linewidth=2)
    plt.axvline(lowcut, color='green', linestyle='--', alpha=0.7)
    plt.axvline(highcut, color='red', linestyle='--', alpha=0.7)
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

df_final = pd.DataFrame()

# Reset index to get Timestamp as a column
df_reset = df.reset_index()
df_final['Timestamp'] = df_reset['Timestamp']

# Add filtered signals (all have same length as df)
for col in axes_accel + gyro_axes:
    df_final[col] = results[col]['filtered']

# Save to CSV
output_file = "beard_pulling_filtered_Person1.csv"
df_final.to_csv(output_file, index=False)
print(f"\n✅ Final filtered IMU data saved to {output_file}")
print(f"   Shape: {df_final.shape}")

# ================== VISUALIZE RAW VS FILTERED ==================
raw_acc = df[accel_axes].values
raw_gyro = df[gyro_axes].values

denoised_acc = df_final[accel_axes].values
denoised_gyro = df_final[gyro_axes].values

axes_labels = ['X', 'Y', 'Z']
t = np.arange(len(df)) / fs

print("\n📊 Visualizing Accelerometer Signals...")
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12, 4))
    plt.plot(t, raw_acc[:, i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:, i], alpha=0.9, linewidth=2, label=f'Filtered Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Filtered')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

print("\n📊 Visualizing Gyroscope Signals...")
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12, 4))
    plt.plot(t, raw_gyro[:, i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:, i], alpha=0.9, linewidth=2, label=f'Filtered Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Filtered')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def calculate_noise_rms(raw, denoised):
    """Calculate RMS of removed noise."""
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

def calculate_snr_improvement(raw, denoised):
    """Calculate SNR improvement in dB."""
    noise_after = raw - denoised
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / (np.var(noise_after, axis=0) + 1e-12))
    return snr_improvement

print("\n📈 Noise Analysis:")
print("=" * 60)

acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

print("\nAccelerometer:")
for i, axis in enumerate(axes_labels):
    print(f"  {axis}-Axis:")
    print(f"    Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"    SNR Improvement: {acc_snr_improvement[i]:.2f} dB")

# Gyroscope
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

print("\nGyroscope:")
for i, axis in enumerate(axes_labels):
    print(f"  {axis}-Axis:")
    print(f"    Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"    SNR Improvement: {gyro_snr_improvement[i]:.2f} dB")

print("\n✅ Preprocessing complete!")

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for nail biting sahil

loading the csv file
"""

df = pd.read_csv('/content/nail biting.csv')
df.head()

"""checking for missing values"""

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

"""converting timestamp to date time format"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

"""Analysing the trend, seasonality and residual of the time series signal"""

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

"""Plotting the pacf for figurring out whether there is a trend or a seasonality in the data"""

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

"""figuring out the sampling frequency of the imu sensor"""

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

"""plotting the power spectral density for analysing noise in the time series signal"""

# Select X-axis columns
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# ==========================
# 2️⃣ Compute PSD using Welch
# ==========================
f_accel, Pxx_accel = welch(accel_x, fs=fs, nperseg=1024)
f_gyro, Pxx_gyro   = welch(gyro_x, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot side-by-side
# ==========================
fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)

# Accelerometer X-axis PSD
axes[0].semilogy(f_accel, Pxx_accel, color='blue')
axes[0].set_title('PSD - AccelerationX')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].grid(True)

# Gyroscope X-axis PSD
axes[1].semilogy(f_gyro, Pxx_gyro, color='red')
axes[1].set_title('PSD - RotationX')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].grid(True)

plt.tight_layout()
plt.show()

"""get the spectral features for qualititatvely analysing the noise in the signal"""

def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    # Normalize PSD for probability interpretation
    Pxx_norm = Pxx / np.sum(Pxx)

    # 1️⃣ Dominant frequency (peak frequency)
    dominant_freq = f[np.argmax(Pxx)]

    # 2️⃣ Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # 3️⃣ Spectral entropy (measure of signal randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # 4️⃣ Band power: energy in specific frequency ranges
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# ⚙️ Compute for both signals
# ==========================
accel_features = spectral_features(f_accel, Pxx_accel)
gyro_features  = spectral_features(f_gyro, Pxx_gyro)
summary_df = pd.DataFrame([accel_features, gyro_features],
                          index=['AccelerationX', 'RotationX'])

print(summary_df)

"""designing a 4th order band pass butterworth filter for removing noise from the data and keeping only meaningful signal"""

# Parameters
lowcut = 0.3   # Hz
highcut = 8.0  # Hz
order = 4      # filter order
fs = fs        # your sampling frequency

# --- Define Butterworth bandpass ---
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    y = filtfilt(b, a, data)  # zero-phase filtering
    return y

# --- Apply to IMU signals ---
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

accel_x_filt = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_x_filt  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

"""comparing raw vs filtered signals for checking whether the noise component has been removed or not?"""

# --- Compare PSD before & after filtering ---
f_raw, Pxx_raw = welch(accel_x, fs=fs, nperseg=1024)
f_filt, Pxx_filt = welch(accel_x_filt, fs=fs, nperseg=1024)

plt.figure(figsize=(8,4))
plt.semilogy(f_raw, Pxx_raw, label='Raw')
plt.semilogy(f_filt, Pxx_filt, label='Filtered')
plt.axvline(lowcut, color='green', linestyle='--', label=f'Lowcut {lowcut}Hz')
plt.axvline(highcut, color='red', linestyle='--', label=f'Highcut {highcut}Hz')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density')
plt.title('AccelerationX PSD: Raw vs Filtered')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""calculatint the spectral features for the denoised signal"""

import numpy as np
from scipy.signal import welch
from scipy.stats import entropy

def compute_spectral_features(signal, fs):
    # Welch PSD
    f, Pxx = welch(signal, fs=fs, nperseg=1024)

    # Normalize PSD for entropy calculation
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency (max power)
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted mean frequency)
    spectral_centroid = np.sum(f * Pxx) / np.sum(Pxx)

    # Spectral entropy
    spec_entropy = entropy(Pxx_norm)

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    bp_0_3 = bandpower(f, Pxx, 0, 3)
    bp_3_6 = bandpower(f, Pxx, 3, 6)
    bp_6_12 = bandpower(f, Pxx, 6, 12)

    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bp_0_3,
        '3-6Hz': bp_3_6,
        '6-12Hz': bp_6_12
    }

# Apply to your filtered signals
features_accel = compute_spectral_features(accel_x_filt, fs)
features_gyro  = compute_spectral_features(gyro_x_filt, fs)

features_df = pd.DataFrame([features_accel, features_gyro],
                           index=['AccelerationX', 'RotationX'])
print(features_df)

"""applying various filtering techniques to see which one gives better results and getting the corresponding plots and qualitative features"""

# ====== Wavelet denoising + feature recompute (ready to run) ======
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt
import pandas as pd
from scipy.signal import butter, filtfilt

# ---------- helper: Butterworth bandpass (reuse if you have it) ----------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def apply_bandpass_filter(data, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, data)

# ---------- wavelet denoise ----------
def wavelet_denoise(x, wavelet='db4', mode='per', level=None, method='universal', threshold_scaling=1.0):
    """
    Wavelet denoising (soft threshold) using pywt.
    - wavelet: wavelet name (db4 is a good default for IMU)
    - mode: signal extension mode
    - level: decomposition level; if None, use pywt.dwt_max_level
    - method: threshold method: 'universal' (VisuShrink) or 'sureshrink'
    - threshold_scaling: multiply computed threshold by this factor (<1 to keep more signal)
    """
    # 1) decompose
    max_level = pywt.dwt_max_level(len(x), pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = max(1, min(max_level, int(np.floor(np.log2(len(x)))) - 1))
    coeffs = pywt.wavedec(x, wavelet, mode=mode, level=level)

    # 2) estimate noise sigma from the finest detail coeffs (median absolute deviation)
    detail_coeffs = coeffs[-1]
    sigma = np.median(np.abs(detail_coeffs)) / 0.6745 + 1e-12

    # 3) compute threshold
    if method == 'universal':
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    elif method == 'sureshrink':
        # pywt provides thresholding methods, but implement simple fallback
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))
    else:
        uthresh = sigma * np.sqrt(2 * np.log(len(x)))

    uthresh *= threshold_scaling

    # 4) threshold detail coefficients (soft)
    denoised_coeffs = coeffs[:1] + [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]
    # 5) reconstruct
    x_denoised = pywt.waverec(denoised_coeffs, wavelet, mode=mode)
    # ensure same length
    x_denoised = x_denoised[:len(x)]
    return x_denoised

# ---------- spectral features ----------
def compute_spectral_features(signal, fs, nperseg=1024):
    f, Pxx = welch(signal, fs=fs, nperseg=nperseg)
    Pxx_sum = np.trapz(Pxx, f) + 1e-12
    Pxx_norm = Pxx / Pxx_sum
    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    # spectral entropy (base e)
    spec_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))
    # band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])
    return {
        'DominantFreq_Hz': dominant_freq,
        'SpectralCentroid_Hz': spectral_centroid,
        'SpectralEntropy': spec_entropy,
        '0-3Hz': bandpower(f, Pxx, 0, 3),
        '3-6Hz': bandpower(f, Pxx, 3, 6),
        '6-12Hz': bandpower(f, Pxx, 6, 12),
        'f': f, 'Pxx': Pxx  # return for optional plotting
    }

# ---------- Parameters ----------------
lowcut = 0.3
highcut = 8.0
order = 4

wavelet = 'db4'
threshold_scaling = 1.0   # <1 keeps more signal; >1 stronger denoising
method = 'universal'      # threshold method

# ---------- Signals ----------
accel_x = df['AccelerationX'].values
gyro_x  = df['RotationX'].values

# 1) Butterworth bandpass (for comparison)
accel_bp = apply_bandpass_filter(accel_x, lowcut, highcut, fs, order)
gyro_bp  = apply_bandpass_filter(gyro_x, lowcut, highcut, fs, order)

# 2) Wavelet denoise (option: denoise raw or bandpass result; below we denoise raw)
accel_wv = wavelet_denoise(accel_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)
gyro_wv  = wavelet_denoise(gyro_x, wavelet=wavelet, threshold_scaling=threshold_scaling, method=method)

# 3) Optionally combine: wavelet -> bandpass (denoise then bandpass)
accel_wv_bp = apply_bandpass_filter(accel_wv, lowcut, highcut, fs, order)
gyro_wv_bp  = apply_bandpass_filter(gyro_wv, lowcut, highcut, fs, order)

# 4) Compute features for all variants
def extract_and_format(signal, name):
    feats = compute_spectral_features(signal, fs)
    # drop f, Pxx from summary; keep for plotting separately
    summary = {k: v for k, v in feats.items() if k not in ('f', 'Pxx')}
    return feats, pd.Series(summary, name=name)

variants = {}
variants['raw_accel'] = extract_and_format(accel_x, 'AccelerationX_raw')
variants['bp_accel']  = extract_and_format(accel_bp, 'AccelerationX_bp')
variants['wv_accel']  = extract_and_format(accel_wv, 'AccelerationX_wv')
variants['wvbp_accel']= extract_and_format(accel_wv_bp, 'AccelerationX_wv_bp')

variants['raw_gyro'] = extract_and_format(gyro_x, 'RotationX_raw')
variants['bp_gyro']  = extract_and_format(gyro_bp, 'RotationX_bp')
variants['wv_gyro']  = extract_and_format(gyro_wv, 'RotationX_wv')
variants['wvbp_gyro']= extract_and_format(gyro_wv_bp, 'RotationX_wv_bp')

# summary table
rows = [variants[k][1] for k in variants]
summary_df = pd.concat(rows, axis=1).T
print(summary_df)

def plot_time_and_psd(raw, bp, wv, wvbp, fs, title_prefix='AccelerationX', variant_key='accel'):
    t = np.arange(len(raw)) / fs
    plt.figure(figsize=(12,5))

    # Time domain
    plt.subplot(1,2,1)
    plt.plot(t, raw, alpha=0.6, label='raw')
    plt.plot(t, bp, alpha=0.8, label='butter_bp')
    plt.plot(t, wvbp, alpha=0.8, label='wavelet->bp')
    plt.xlim(0, min(10, t[-1]))
    plt.xlabel('Time (s)')
    plt.ylabel('Signal')
    plt.title(f'{title_prefix} - Time domain (zoom)')
    plt.legend()
    plt.grid(True)

    # PSD domain
    plt.subplot(1,2,2)
    f_raw, P_raw   = variants[f'raw_{variant_key}'][0]['f'], variants[f'raw_{variant_key}'][0]['Pxx']
    f_bp, P_bp     = variants[f'bp_{variant_key}'][0]['f'],  variants[f'bp_{variant_key}'][0]['Pxx']
    f_wv, P_wv     = variants[f'wv_{variant_key}'][0]['f'],  variants[f'wv_{variant_key}'][0]['Pxx']
    f_wvbp, P_wvbp = variants[f'wvbp_{variant_key}'][0]['f'], variants[f'wvbp_{variant_key}'][0]['Pxx']

    plt.semilogy(f_raw, P_raw, label='raw')
    plt.semilogy(f_bp, P_bp, label='butter_bp')
    plt.semilogy(f_wv, P_wv, label='wavelet')
    plt.semilogy(f_wvbp, P_wvbp, label='wavelet->bp')
    plt.axvline(lowcut, color='green', linestyle='--', label=f'lowcut {lowcut}Hz')
    plt.axvline(highcut, color='red', linestyle='--', label=f'highcut {highcut}Hz')
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'{title_prefix} - PSD comparison')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""comparing the results of the techniques used above by looking at the plots"""

plot_time_and_psd(accel_x, accel_bp, accel_wv, accel_wv_bp, fs, title_prefix='AccelerationX')
plot_time_and_psd(gyro_x, gyro_bp, gyro_wv, gyro_wv_bp, fs, title_prefix='RotationX')

print(summary_df[['DominantFreq_Hz', 'SpectralEntropy', 'SpectralCentroid_Hz', '0-3Hz', '3-6Hz', '6-12Hz']])

def compute_noise_index(f, Pxx, signal_band=(0.3, 10), noise_band=(15, np.inf)):
    # Boolean masks for each frequency band
    sig_mask = (f >= signal_band[0]) & (f <= signal_band[1])
    noise_mask = (f >= noise_band[0])

    # Integrate only the selected portions
    signal_power = np.trapezoid(Pxx[sig_mask], f[sig_mask])
    noise_power  = np.trapezoid(Pxx[noise_mask], f[noise_mask])

    return noise_power / (signal_power + 1e-12)

for k, (feats, _) in variants.items():
    ni = compute_noise_index(feats['f'], feats['Pxx'])
    print(f"{k:15s}  Noise Index = {ni:.4e}")

signals = {
    "Raw": accel_x,
    "Bandpass": accel_bp,
    "Wavelet": accel_wv,
    "Wavelet+BP": accel_wv_bp,
}


plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (AccelerationX)")
plt.legend()
plt.grid(True)
plt.show()

signals = {
    "Raw": gyro_x,
    "Bandpass": gyro_bp,
    "Wavelet": gyro_wv,
    "Wavelet+BP": gyro_wv_bp,
}

plt.figure(figsize=(12, 6))
for name, sig in signals.items():
    plt.plot(sig[:1000], label=name, alpha=0.7)
plt.title("Comparison of Filtered Signals (RotationX)")
plt.legend()
plt.grid(True)
plt.show()

# Accelerometer
accel_wv_y = wavelet_denoise(df['AccelerationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
accel_wv_z = wavelet_denoise(df['AccelerationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

accel_wv_bp_y = apply_bandpass_filter(accel_wv_y, lowcut, highcut, fs, order)
accel_wv_bp_z = apply_bandpass_filter(accel_wv_z, lowcut, highcut, fs, order)

# Gyroscope
gyro_wv_y = wavelet_denoise(df['RotationY'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)
gyro_wv_z = wavelet_denoise(df['RotationZ'].values, wavelet=wavelet, threshold_scaling=threshold_scaling)

gyro_wv_bp_y = apply_bandpass_filter(gyro_wv_y, lowcut, highcut, fs, order)
gyro_wv_bp_z = apply_bandpass_filter(gyro_wv_z, lowcut, highcut, fs, order)

"""storing the best denoised signals in the dataframe for further processing"""

df_denoised = pd.DataFrame({
    'AccelerationX': accel_wv_bp,
    'AccelerationY': accel_wv_bp_y,
    'AccelerationZ': accel_wv_bp_z,
    'RotationX': gyro_wv_bp,
    'RotationY': gyro_wv_bp_y,
    'RotationZ': gyro_wv_bp_z
})


# Optional: include timestamp if you have it
# df_denoised['Timestamp'] = df['Timestamp']

# Save to CSV
df_denoised.to_csv("denoised_signal_nail_biting.csv", index=False)

print("✅ Wavelet + bandpass denoised dataframe stored successfully as CSV.")

df_denoised.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_denoised[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_denoised[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

"""#Data preprocessing for hair pulling sahil"""

df = pd.read_csv('/content/hair pulling.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

plt.figure(figsize=(8,4))
plot_pacf(imu_data['AccelerationX'], lags=100, method='ywm')
plt.title("Partial Autocorrelation - AccelerationX")
plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.3
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.3, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_hair.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for face Itching sahil

"""

df = pd.read_csv('/content/itching on face.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_pacf

# List of signal columns
acc_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes = ['RotationX', 'RotationY', 'RotationZ']

# ---- Accelerometer PACF ----
for axis in acc_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# ---- Gyroscope PACF ----
for axis in gyro_axes:
    plt.figure(figsize=(8,4))
    plot_pacf(imu_data[axis], lags=100, method='ywm')
    plt.title(f"Partial Autocorrelation - {axis}")
    plt.xlabel("Lag")
    plt.ylabel("PACF")
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1
    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised

# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a

def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return filtfilt(b, a, x)

def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f

# -------------------------------
# Parameters
# -------------------------------
fs = 50
axes_accel = ['AccelerationX','AccelerationY','AccelerationZ']
axes_gyro  = ['RotationX','RotationY','RotationZ']
lowcut = 0.1
highcut_accel = 6.0
highcut_gyro  = 6.0

# -------------------------------
# Apply Denoise + Bandpass to All Axes
# -------------------------------
results = {}
for col in axes_accel + axes_gyro:
    sig = df[col].values
    highcut = highcut_accel if 'Acceleration' in col else highcut_gyro
    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

# -------------------------------
# Plot PSD Comparison for All Axes
# -------------------------------
for col in axes_accel + axes_gyro:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    plt.figure(figsize=(8,4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.7)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.7)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col==axes_accel[0] else "")
    highcut_val = highcut_accel if 'Acceleration' in col else highcut_gyro
    plt.axvline(highcut_val, color='red', linestyle='--', label='Highcut' if col==axes_accel[0] else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
from scipy.signal import welch

def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapz(Pxx, f)
    band = np.trapz(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # avoid division by zero

# Parameters
lowcut, highcut = 0.1, 6.0
axes_all = axes_accel + axes_gyro

# Compute passband fraction for all axes
passband_results = {}

for col in axes_all:
    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# Print results neatly
for col in axes_all:
    print(f"{col}:")
    print(f"  raw passband frac: {passband_results[col]['raw']:.6f}")
    print(f"  denoised passband frac: {passband_results[col]['denoised']:.6f}")
    print(f"  filtered passband frac: {passband_results[col]['filtered']:.6f}\n")

# Columns to keep (accelerations + rotations)
columns_to_keep = ['AccelerationX','AccelerationY','AccelerationZ',
                   'RotationX','RotationY','RotationZ']

# Build the DataFrame with filtered signals
df_final = pd.DataFrame({col: results[col]['filtered'] for col in columns_to_keep},
                        index=df.index)  # keep the Timestamp index

# Save to CSV
df_final.to_csv('denoised_signals_filtered_face_itching.csv')

print("Final CSV saved with timestamp as index and filtered sensor axes only.")

df_final.head()

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Data Preprocessing for beard pulling  sahil

"""

df = pd.read_csv('/content/beard pulling.csv')
df.head()

# Check for missing values in each column
missing_values = df.isnull().sum()

print("Missing values per column:\n", missing_values)

df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')  # adjust 'unit' if needed
df = df.sort_values('Timestamp')
df = df.set_index('Timestamp')

df.head()

imu_cols = ['AccelerationX', 'AccelerationY', 'AccelerationZ',
            'RotationX', 'RotationY', 'RotationZ']

imu_data = df[imu_cols]

# Estimate the sampling rate if known, e.g., 50 Hz
# The period for decomposition should represent one "cycle" of motion — you can start with 50 (1 second)
result = seasonal_decompose(imu_data['AccelerationX'], model='additive', period=50)

# Plot the decomposition
result.plot()
plt.suptitle("Decomposition of AccelerationX", fontsize=14)
plt.show()

for col in imu_cols:
    result = seasonal_decompose(imu_data[col], model='additive', period=50)
    result.plot()
    plt.suptitle(f"Decomposition of {col}", fontsize=14)
    plt.show()

# Compute time differences between consecutive samples
dt = np.diff(df.index.values.astype(np.int64) / 1e9)  # convert ns → seconds
fs = 1.0 / np.mean(dt)
print(f"Estimated Sampling Rate: {fs:.2f} Hz")

import matplotlib.pyplot as plt
from scipy.signal import welch

# ==========================
# 1️⃣ Extract IMU Columns
# ==========================
accel_x = df['AccelerationX'].values
accel_y = df['AccelerationY'].values
accel_z = df['AccelerationZ'].values

gyro_x = df['RotationX'].values
gyro_y = df['RotationY'].values
gyro_z = df['RotationZ'].values

# ==========================
# 2️⃣ Compute PSD using Welch Method
# ==========================
f_accel_x, Pxx_accel_x = welch(accel_x, fs=fs, nperseg=1024)
f_accel_y, Pxx_accel_y = welch(accel_y, fs=fs, nperseg=1024)
f_accel_z, Pxx_accel_z = welch(accel_z, fs=fs, nperseg=1024)

f_gyro_x, Pxx_gyro_x = welch(gyro_x, fs=fs, nperseg=1024)
f_gyro_y, Pxx_gyro_y = welch(gyro_y, fs=fs, nperseg=1024)
f_gyro_z, Pxx_gyro_z = welch(gyro_z, fs=fs, nperseg=1024)

# ==========================
# 3️⃣ Plot: Accelerometer & Gyroscope PSDs
# ==========================
fig, axes = plt.subplots(2, 1, figsize=(12, 10))

# --- Accelerometer ---
axes[0].semilogy(f_accel_x, Pxx_accel_x, label='Accel X', color='tab:blue')
axes[0].semilogy(f_accel_y, Pxx_accel_y, label='Accel Y', color='tab:orange')
axes[0].semilogy(f_accel_z, Pxx_accel_z, label='Accel Z', color='tab:green')
axes[0].set_title('Accelerometer PSD (X, Y, Z)')
axes[0].set_xlabel('Frequency (Hz)')
axes[0].set_ylabel('Power Spectral Density')
axes[0].legend()
axes[0].grid(True)

# --- Gyroscope ---
axes[1].semilogy(f_gyro_x, Pxx_gyro_x, label='Gyro X', color='tab:red')
axes[1].semilogy(f_gyro_y, Pxx_gyro_y, label='Gyro Y', color='tab:purple')
axes[1].semilogy(f_gyro_z, Pxx_gyro_z, label='Gyro Z', color='tab:brown')
axes[1].set_title('Gyroscope PSD (X, Y, Z)')
axes[1].set_xlabel('Frequency (Hz)')
axes[1].set_ylabel('Power Spectral Density')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from scipy.signal import welch

# ==========================
# Define function (as you already did)
# ==========================
def spectral_features(f, Pxx, band_limits=[(0,3), (3,6), (6,12)]):
    """Compute key spectral features from PSD."""
    Pxx_norm = Pxx / np.sum(Pxx)

    # Dominant frequency
    dominant_freq = f[np.argmax(Pxx)]

    # Spectral centroid (weighted average frequency)
    spectral_centroid = np.sum(f * Pxx_norm)

    # Spectral entropy (measure of randomness)
    spectral_entropy = -np.sum(Pxx_norm * np.log2(Pxx_norm + 1e-12))

    # Band power
    band_powers = {}
    for (low, high) in band_limits:
        idx = np.logical_and(f >= low, f <= high)
        band_powers[f"{low}-{high}Hz"] = np.trapz(Pxx[idx], f[idx])

    return {
        "DominantFreq_Hz": dominant_freq,
        "SpectralCentroid_Hz": spectral_centroid,
        "SpectralEntropy": spectral_entropy,
        **band_powers
    }

# ==========================
# Extract IMU columns
# ==========================
accel_axes = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
gyro_axes  = ['RotationX', 'RotationY', 'RotationZ']

# ==========================
# Compute PSD + spectral features for all axes
# ==========================
features = {}

for col in accel_axes + gyro_axes:
    signal = df[col].values
    f, Pxx = welch(signal, fs=fs, nperseg=1024)
    features[col] = spectral_features(f, Pxx)

# ==========================
# Combine into DataFrame
# ==========================
summary_df = pd.DataFrame(features).T
summary_df

import numpy as np
import pywt
from scipy.signal import butter, filtfilt, welch
import matplotlib.pyplot as plt

# -------------------------------
# Wavelet Denoising
# -------------------------------
def wavelet_denoise(x, wavelet='db4', level=None, mode='soft'):
    n = x.size
    max_lev = pywt.dwt_max_level(n, pywt.Wavelet(wavelet).dec_len)
    if level is None:
        level = min(5, max_lev) if max_lev >= 1 else 1

    coeffs = pywt.wavedec(x, wavelet, level=level, mode='symmetric')
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    uthresh = sigma * np.sqrt(2 * np.log(n))
    coeffs_thresh = [coeffs[0]] + [
        pywt.threshold(c, uthresh, mode=mode) for c in coeffs[1:]
    ]
    x_denoised = pywt.waverec(coeffs_thresh, wavelet, mode='symmetric')

    # Adjust length if needed
    if x_denoised.size > n:
        x_denoised = x_denoised[:n]
    elif x_denoised.size < n:
        x_denoised = np.pad(x_denoised, (0, n - x_denoised.size), mode='symmetric')
    return x_denoised


# -------------------------------
# Butterworth Bandpass Filter
# -------------------------------
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='bandpass')
    return b, a


def apply_bandpass(x, lowcut, highcut, fs, order=4):
    b, a = butter_bandpass(lowcut, highcut, fs, order)
    return filtfilt(b, a, x)


# -------------------------------
# Combined Wavelet + Bandpass
# -------------------------------
def denoise_then_bandpass(x, fs, wavelet='db4', level=None,
                          lowcut=0.3, highcut=6.0, order=4):
    x_w = wavelet_denoise(x, wavelet=wavelet, level=level)
    x_f = apply_bandpass(x_w, lowcut, highcut, fs, order=order)
    return x_w, x_f


# -------------------------------
# Band Power Fraction Helper
# -------------------------------
def band_power_fraction(f, Pxx, low, high):
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapezoid(Pxx, f)
    band = np.trapezoid(Pxx[idx], f[idx])
    return band / (total + 1e-20)


# -------------------------------
# Parameters
# -------------------------------
fs = 50  # sampling rate

# Optimized bandpass per axis (from your spectral data)
filter_params = {
    'AccelerationX': (0.3, 10.0),
    'AccelerationY': (0.3, 10.0),
    'AccelerationZ': (0.3, 12.0),
    'RotationX':     (0.3, 6.0),
    'RotationY':     (0.3, 6.0),
    'RotationZ':     (0.3, 6.0),
}

axes_all = list(filter_params.keys())

# -------------------------------
# Apply to all signals
# -------------------------------
results = {}
for col in axes_all:
    lowcut, highcut = filter_params[col]
    sig = df[col].values

    deno, filtered = denoise_then_bandpass(sig, fs,
                                           wavelet='db4', level=None,
                                           lowcut=lowcut, highcut=highcut, order=4)
    results[col] = {'raw': sig, 'denoised': deno, 'filtered': filtered}

    # --- PSD and passband fraction ---
    f_raw, P_raw = welch(sig, fs=fs, nperseg=1024)
    f_den, P_den = welch(deno, fs=fs, nperseg=1024)
    f_fil, P_fil = welch(filtered, fs=fs, nperseg=1024)

    frac_raw = band_power_fraction(f_raw, P_raw, lowcut, highcut)
    frac_fil = band_power_fraction(f_fil, P_fil, lowcut, highcut)

    print(f"{col}:  low={lowcut:.1f}Hz  high={highcut:.1f}Hz  |  "
          f"raw_frac={frac_raw:.3f}  filtered_frac={frac_fil:.3f}")

    # --- PSD plot ---
    plt.figure(figsize=(8, 4))
    plt.semilogy(f_raw, P_raw, label='raw', alpha=0.6)
    plt.semilogy(f_den, P_den, label='wavelet denoised', alpha=0.6)
    plt.semilogy(f_fil, P_fil, label='denoise + bandpass', alpha=0.9)
    plt.axvline(lowcut, color='green', linestyle='--', label='Lowcut' if col == 'AccelerationX' else "")
    plt.axvline(highcut, color='red', linestyle='--', label='Highcut' if col == 'AccelerationX' else "")
    plt.xlabel('Frequency (Hz)')
    plt.ylabel('PSD')
    plt.title(f'PSD Comparison - {col}')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import numpy as np
from scipy.signal import welch

# ------------------------------------
# Band Power Fraction (updated)
# ------------------------------------
def band_power_fraction(f, Pxx, low, high):
    """Compute fraction of signal power within a given frequency band."""
    idx = np.logical_and(f >= low, f <= high)
    total = np.trapezoid(Pxx, f)
    band = np.trapezoid(Pxx[idx], f[idx])
    return band / (total + 1e-20)  # Avoid division by zero


# ------------------------------------
# Frequency bands per axis
# (Based on earlier spectral feature table)
# ------------------------------------
filter_params = {
    'AccelerationX': (0.3, 10.0),
    'AccelerationY': (0.3, 10.0),
    'AccelerationZ': (0.3, 12.0),
    'RotationX':     (0.3, 6.0),
    'RotationY':     (0.3, 6.0),
    'RotationZ':     (0.3, 6.0),
}

axes_all = list(filter_params.keys())

# ------------------------------------
# Compute passband fraction for all axes
# ------------------------------------
passband_results = {}

for col in axes_all:
    lowcut, highcut = filter_params[col]

    f_raw, P_raw = welch(results[col]['raw'], fs=fs, nperseg=1024)
    f_den, P_den = welch(results[col]['denoised'], fs=fs, nperseg=1024)
    f_fil, P_fil = welch(results[col]['filtered'], fs=fs, nperseg=1024)

    passband_results[col] = {
        'lowcut': lowcut,
        'highcut': highcut,
        'raw': band_power_fraction(f_raw, P_raw, lowcut, highcut),
        'denoised': band_power_fraction(f_den, P_den, lowcut, highcut),
        'filtered': band_power_fraction(f_fil, P_fil, lowcut, highcut)
    }

# ------------------------------------
# Print results neatly
# ------------------------------------
print("Passband Power Fractions by Axis\n" + "-" * 45)
for col in axes_all:
    vals = passband_results[col]
    print(f"{col}:  [{vals['lowcut']} - {vals['highcut']} Hz]")
    print(f"  raw passband frac:      {vals['raw']:.6f}")
    print(f"  denoised passband frac: {vals['denoised']:.6f}")
    print(f"  filtered passband frac: {vals['filtered']:.6f}\n")

df_raw = pd.read_csv("/content/beard pulling.csv")

df_raw.head()

# Define the IMU signal column names
axes_accel = ['AccelerationX', 'AccelerationY', 'AccelerationZ']
axes_gyro  = ['RotationX', 'RotationY', 'RotationZ']

# --- 1️⃣ Create new DataFrame with timestamps ---
df_final = pd.DataFrame()
df_final['Timestamp'] = df_raw['Timestamp']  # preserve original timestamps

# --- 2️⃣ Add final filtered accelerometer and gyroscope data ---
for col in axes_accel + axes_gyro:
    df_final[col] = results[col]['filtered']

# --- 3️⃣ Save to CSV ---
output_file = "beard_pulling_filtered.csv"
df_final.to_csv(output_file, index=False)

print(f"✅ Final filtered IMU data saved to {output_file}")

# Signals
raw_acc = df[['AccelerationX','AccelerationY','AccelerationZ']].values
raw_gyro = df[['RotationX','RotationY','RotationZ']].values

denoised_acc = df_final[['AccelerationX','AccelerationY','AccelerationZ']].values
denoised_gyro = df_final[['RotationX','RotationY','RotationZ']].values

axes_labels = ['X', 'Y', 'Z']
fs = 50  # sampling frequency (Hz)
t = np.arange(len(df)) / fs

# ---- Accelerometer ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_acc[:,i], alpha=0.6, label=f'Raw Accel {axis}')
    plt.plot(t, denoised_acc[:,i], alpha=0.8, label=f'Denoised Accel {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration (m/s²)')
    plt.title(f'Accelerometer {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# ---- Gyroscope ----
for i, axis in enumerate(axes_labels):
    plt.figure(figsize=(12,4))
    plt.plot(t, raw_gyro[:,i], alpha=0.6, label=f'Raw Gyro {axis}')
    plt.plot(t, denoised_gyro[:,i], alpha=0.8, label=f'Denoised Gyro {axis}')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.title(f'Gyroscope {axis}-Axis: Raw vs Denoised')
    plt.legend()
    plt.grid(True)
    plt.show()

# Function to calculate RMS of noise
def calculate_noise_rms(raw, denoised):
    noise = raw - denoised
    rms = np.sqrt(np.mean(noise**2, axis=0))
    return rms

# Function to calculate SNR improvement (dB)
def calculate_snr_improvement(raw, denoised):
    noise_before = raw - raw  # effectively zero
    noise_after = raw - denoised

    # Variance-based SNR improvement
    snr_improvement = 10 * np.log10(np.var(raw, axis=0) / np.var(noise_after, axis=0))
    return snr_improvement

axes_labels = ['X', 'Y', 'Z']

# ---- Accelerometer ----
acc_noise_rms = calculate_noise_rms(raw_acc, denoised_acc)
acc_snr_improvement = calculate_snr_improvement(raw_acc, denoised_acc)

for i, axis in enumerate(axes_labels):
    print(f"Accelerometer {axis}-Axis:")
    print(f"  Noise RMS: {acc_noise_rms[i]:.4f} m/s²")
    print(f"  SNR Improvement: {acc_snr_improvement[i]:.2f} dB\n")

# ---- Gyroscope ----
gyro_noise_rms = calculate_noise_rms(raw_gyro, denoised_gyro)
gyro_snr_improvement = calculate_snr_improvement(raw_gyro, denoised_gyro)

for i, axis in enumerate(axes_labels):
    print(f"Gyroscope {axis}-Axis:")
    print(f"  Noise RMS: {gyro_noise_rms[i]:.4f} rad/s")
    print(f"  SNR Improvement: {gyro_snr_improvement[i]:.2f} dB\n")

"""# Feature Engineering for nail biting Ananya"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_nail_biting.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_nail_biting_Ananya.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for hair pulling Ananya"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_hair_pulling.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_hair_pulling_Ananya.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for face itcing Ananya

"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_face_scratching.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_face_itching_Ananya.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for nail biting Person2"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person2.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_nail_biting_Person2.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for hair pulling Person2"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person2.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_hair_pulling_Person2.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for face itcing Perosn2

"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Face_scratching_person1.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_face_itching_Person2.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for beard pulling Person2

"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person2.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_beard_pulling_Peron2.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for nail biting Person 1"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person1.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_nail_biting_Person1.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for hair pulling Person1"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person1.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_hair_pulling_Person1.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for face itcing Person1

"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Face_Scratching_person2.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_face_itching_Person1.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for beard pulling Pesron1

"""

df_denoised = pd.read_csv('/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person1.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_beard_pulling_Person1.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for nail biting sahil"""

df_denoised = pd.read_csv('/content/denoised_signal_nail_biting.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_nail_biting.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for hair pulling sahil"""

df_denoised = pd.read_csv('/content/denoised_signals_filtered_hair.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_hair_pulling.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for face itcing sahil

"""

df_denoised = pd.read_csv('/content/denoised_signals_filtered_face_itching.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_face_itching.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Feature Engineering for beard pulling sahil

"""

df_denoised = pd.read_csv('/content/beard_pulling_filtered.csv')

# Sampling rate
fs = 50  # Hz, adjust if needed

# Window parameters
window_duration = 2  # seconds
window_size = window_duration * fs
step_size = window_size // 2  # 50% overlap

def compute_spatial_features(segment, sensor_prefix):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)

    feats = {
        'mean_x': np.mean(x),
        'mean_y': np.mean(y),
        'mean_z': np.mean(z),
        'std_x': np.std(x),
        'std_y': np.std(y),
        'std_z': np.std(z),
        'rms': np.sqrt(np.mean(mag**2)),
        'mean_mag': np.mean(mag),
        'std_mag': np.std(mag),
        'energy': np.sum(mag**2),
        'corr_xy': np.corrcoef(x, y)[0,1],
        'corr_yz': np.corrcoef(y, z)[0,1],
        'corr_zx': np.corrcoef(z, x)[0,1]
    }

    return pd.Series(feats)

from scipy.signal import welch

def compute_spectral_features(segment, sensor_prefix, fs):
    x = segment[f'{sensor_prefix}X'].values
    y = segment[f'{sensor_prefix}Y'].values
    z = segment[f'{sensor_prefix}Z'].values

    mag = np.sqrt(x**2 + y**2 + z**2)
    f, Pxx = welch(mag, fs=fs, nperseg=min(1024, len(mag)))
    Pxx_norm = Pxx / (np.sum(Pxx) + 1e-12)

    dominant_freq = f[np.argmax(Pxx)]
    spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12)
    spectral_entropy = -np.sum(Pxx_norm * np.log(Pxx_norm + 1e-12))

    # Band powers
    def bandpower(f, Pxx, fmin, fmax):
        idx = np.logical_and(f >= fmin, f <= fmax)
        return np.trapz(Pxx[idx], f[idx])

    feats = {
        'dominant_freq': dominant_freq,
        'spectral_centroid': spectral_centroid,
        'spectral_entropy': spectral_entropy,
        '0_3Hz': bandpower(f, Pxx, 0, 3),
        '3_6Hz': bandpower(f, Pxx, 3, 6),
        '6_12Hz': bandpower(f, Pxx, 6, 12)
    }

    return pd.Series(feats)

segments = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    segment = df_denoised.iloc[start:end]

    # Accelerometer features
    accel_spatial = compute_spatial_features(segment, 'Acceleration').add_prefix('acc_')
    accel_spectral = compute_spectral_features(segment, 'Acceleration', fs).add_prefix('acc_')

    # Gyroscope features
    gyro_spatial = compute_spatial_features(segment, 'Rotation').add_prefix('gyro_')
    gyro_spectral = compute_spectral_features(segment, 'Rotation', fs).add_prefix('gyro_')

    # Combine all features
    combined_features = pd.concat([accel_spatial, accel_spectral, gyro_spatial, gyro_spectral])

    # Optional: store window indices for labeling
    combined_features['start_idx'] = start
    combined_features['end_idx'] = end

    segments.append(combined_features)

# Convert to DataFrame
features_df = pd.DataFrame(segments)
print("✅ Feature matrix shape:", features_df.shape)

features_df.head()

# Signals to analyze
signals = {
    'accel_x': df_denoised['AccelerationX'].values,
    'accel_y': df_denoised['AccelerationY'].values,
    'accel_z': df_denoised['AccelerationZ'].values,
    'gyro_x': df_denoised['RotationX'].values,
    'gyro_y': df_denoised['RotationY'].values,
    'gyro_z': df_denoised['RotationZ'].values
}

rms_list = []
fft_list = []

for start in range(0, len(df_denoised) - window_size + 1, step_size):
    end = start + window_size
    window_features = {}

    for key, sig in signals.items():
        segment = sig[start:end]

        # RMS
        window_features[f'{key}_rms'] = np.sqrt(np.mean(segment**2))

        # FFT
        fft_vals = np.fft.rfft(segment)  # real FFT
        fft_freqs = np.fft.rfftfreq(len(segment), d=1/fs)

        # Store FFT magnitude
        window_features[f'{key}_fft'] = np.abs(fft_vals)
        window_features[f'{key}_fft_freqs'] = fft_freqs

    rms_list.append({k:v for k,v in window_features.items() if '_rms' in k})
    fft_list.append({k:v for k,v in window_features.items() if '_fft' in k})

rms_df = pd.DataFrame(rms_list)
print("RMS feature matrix shape:", rms_df.shape)
rms_df.head()

# FFT is a bit trickier because each window has an array of magnitudes
# You can store as a list column or compute FFT features (dominant freq, spectral centroid, band power)
fft_df = pd.DataFrame(fft_list)
print("FFT magnitude list per window (first 5 rows):")
fft_df.head()

rms_fft_df = pd.concat([rms_df, fft_df], axis=1)
print("✅ RMS/FFT feature DataFrame shape:", rms_fft_df.shape)
rms_fft_df.head()

def extract_fft_features(fft_vals, fft_freqs):
    """Compute FFT-based spectral features for one signal segment."""
    mag = np.abs(fft_vals)
    mag_norm = mag / (np.sum(mag) + 1e-12)

    dominant_freq = fft_freqs[np.argmax(mag)]
    spectral_centroid = np.sum(fft_freqs * mag) / (np.sum(mag) + 1e-12)
    spectral_entropy = -np.sum(mag_norm * np.log(mag_norm + 1e-12))

    return dominant_freq, spectral_centroid, spectral_entropy


def compute_spectral_fft_features(df, window_size=256, sampling_rate=50):
    """
    Compute FFT-based spectral features for each axis in denoised IMU data using sliding windows.
    """
    step_size = window_size // 2
    fft_feature_list = []

    for start in range(0, len(df) - window_size, step_size):
        end = start + window_size
        window = df.iloc[start:end]

        features = {}
        for col in ['AccelerationX', 'AccelerationY', 'AccelerationZ',
                    'RotationX', 'RotationY', 'RotationZ']:

            signal = window[col].values
            fft_vals = np.fft.rfft(signal)
            fft_freqs = np.fft.rfftfreq(len(signal), 1 / sampling_rate)

            dominant, centroid, entropy = extract_fft_features(fft_vals, fft_freqs)
            features[f'{col}_DominantFreq'] = dominant
            features[f'{col}_SpectralCentroid'] = centroid
            features[f'{col}_SpectralEntropy'] = entropy

        fft_feature_list.append(features)

    return pd.DataFrame(fft_feature_list)

# Compute FFT spectral features
spectral_fft_df = compute_spectral_fft_features(df_denoised, window_size=256, sampling_rate=50)
print("✅ Spectral FFT feature DataFrame shape:", spectral_fft_df.shape)
spectral_fft_df.head()

combined_features_df = pd.concat([features_df.reset_index(drop=True),
                                  spectral_fft_df.reset_index(drop=True),
                                  rms_fft_df.reset_index(drop=True)], axis=1)

print("✅ Combined feature dataframe shape:", combined_features_df.shape)
combined_features_df.head()

# Save the updated denoised file
combined_features_df.to_csv("final_features_beard_pulling.csv", index=False)
print("✅ Combined features dataframe stored successfully as CSV.")

num_features = combined_features_df.shape[1]
print("Number of features extracted:", num_features)

print("The extracted features are : ", combined_features_df.columns.tolist())

"""# Labelling the data for nail biting Ananya

"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_nail_biting.csv")
denoised_df = pd.read_csv("/content/denoised_signal_nail_biting_Ananya.csv")

print(len(original_df))
print(len(denoised_df))

# Copy timestamp column
denoised_df["Timestamp"] = original_df["Timestamp"].values

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_nail_biting_withTime_Ananya.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("denoised_signal_for_nail_biting_withTime_Ananya.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/nail_biting/girl_nail.xlsx")

# Save as CSV
df.to_csv("/content/labelled_nail_biting_Ananya.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

df.head()

# Load the file and check what columns it actually has
ann = pd.read_csv("/content/labelled_nail_biting_Ananya.csv")
print("Column names in the file:")
print(ann.columns.tolist())
print("\nFirst few rows:")
print(ann.head())

# Load annotations
ann = pd.read_csv("/content/labelled_nail_biting_Ananya.csv")
# Apply offset (change this to your computed value)
offset = 4.46
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_nail_biting_Ananya.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_nail_biting_Ananya.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_nail_biting_Ananya.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_nail_biting_Ananya.csv")
# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.51  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_nail_biting_Ananya.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_nail_biting.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_nail_biting_Ananya.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'nail': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_nail_biting_Ananya.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_nail_biting_Ananya.csv")
df.head()

"""# Labelling the data for hair pulling Ananya


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_hair_pulling.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_hair_Ananya.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling_Ananya.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_hair_pulling_withTime_Ananya.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_hair_pulling_withTime_Ananya.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/hair_pulling/girl_hair.xlsx")

# Save as CSV
df.to_csv("/content/labelled_hair_pulling_Ananya.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_hair_pulling_Ananya.csv")

# Apply offset (change this to your computed value)
offset = 1.26
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_hair__pulling_Ananya.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_hair__pulling_Ananya.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_hair_pulling_Ananya.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_hair_pulling_Ananya.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 1.26  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_hair_pulling_Ananya.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_hair_pulling.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_hair_pulling_Ananya.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'hair': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_hair_pulling_Ananya.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_hair_pulling_Ananya.csv")
df.head()

"""# Labelling the data for face itching Ananya


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Ananya_face_scratching.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_face_itching_Ananya.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_face_itching_Ananya.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_face_itching_withTime_Ananya.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_face_itching_withTime_Ananya.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/face_itching/girl_face.xlsx")

# Save as CSV
df.to_csv("/content/labelled_face_itching_Ananya.csv", index=False)

print("✅ File converted and saved as labelled_face_itching.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_face_itching_Ananya.csv")

# Apply offset (change this to your computed value)
offset = 0.64
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_face_itching_Ananya.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_face_itching_Ananya.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_face_itching_Ananya.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_face_itching_Ananya.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.64  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_face_itching_Ananya.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_face_itching_Ananya.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_face_itching_Ananya.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'face': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_face_itching_Ananya.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_face_itching_Ananya.csv")
df.head()

"""# Labelling the data for nail biting Peron2

"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person2.csv")
denoised_df = pd.read_csv("/content/denoised_signal_nail_biting_Person2.csv")

print(len(original_df))
print(len(denoised_df))

# Copy timestamp column
denoised_df["Timestamp"] = original_df["Timestamp"].values

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_nail_biting_withTime_Person2.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("denoised_signal_for_nail_biting_withTime_Person2.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/nail_biting/himanshu_nail_1.xlsx")

# Save as CSV
df.to_csv("/content/labelled_nail_biting_Person2.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_nail_biting_Person2.csv")

# Apply offset (change this to your computed value)
offset = 1.72
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_nail_biting_Person2.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_nail_biting_Person2.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_nail_biting_Person2.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_nail_biting_Person2.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 1.72  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_nail_biting_Person2.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_nail_biting.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_nail_biting_Person2.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'nail': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_nail_biting_Person2.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_nail_biting_Person2.csv")
df.head()

"""# Labelling the data for hair pulling Person2


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person2.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_hair_Person2.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling_Person2.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_hair_pulling_withTime_Person2.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_hair_pulling_withTime_Person2.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/hair_pulling/himanshu_Hair_1.xlsx")

# Save as CSV
df.to_csv("/content/labelled_hair_pulling_Person2.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting_Person.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_hair_pulling_Person2.csv")

# Apply offset (change this to your computed value)
offset = 1.74
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_hair__pulling.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_hair__pulling_Person2.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_hair_pulling_Person2.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_hair_pulling_Person2.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 1.74  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_hair_pulling_Person2.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_hair_pulling_Person2.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_hair_pulling_Person2.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'hair': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_hair_pulling_Person2.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_hair_pulling_Person2.csv")
df.head()

"""# Labelling the data for face itching Person2


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Face_scratching_person1.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_face_itching_Person2.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_face_itching_Person2.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_face_itching_withTime_Person2.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_face_itching_withTime_Person2.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/face_itching/himanshu_face_1.xlsx")

# Save as CSV
df.to_csv("/content/labelled_face_itching_Person2.csv", index=False)

print("✅ File converted and saved as labelled_face_itching.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_face_itching_Person2.csv")

# Apply offset (change this to your computed value)
offset = 0.19
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_face_itching_Person2.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_face_itching_Person2.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_face_itching_Person2.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_face_itching_Person2.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.19 # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_face_itching_Person2.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_face_itching.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_face_itching_Person2.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'face': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_face_itching_Person2.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_face_itching_Person2.csv")
df.head()

"""# Labelling the data for beard pulling Person2


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person2.csv")
denoised_df = pd.read_csv("/content/beard_pulling_filtered_Person2.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling_Person2.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_beard_pulling_withTime_Person2.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_beard_pulling_withTime_Person2.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/beard_pulling/himanshu_beard_1.xlsx")

# Save as CSV
df.to_csv("/content/labelled_beard_pulling_Person2.csv", index=False)

print("✅ File converted and saved as labelled_beard_pulling.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_beard_pulling_Person2.csv")

# Apply offset (change this to your computed value)
offset = 1.42
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_beard_pulling_Person2.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_beard_pulling_Person2.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_beard_pulling_Peron2.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_beard_pulling_Person2.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 1.42  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_beard_pulling_Person2.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_beard_pulling.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_beard_pulling_Person2.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'beard': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_beard_pulling_Person2.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_beard_pulling_Person2.csv")
df.head()

"""# Labelling the data for nail biting Person1

"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Nail_biting_person1.csv")
denoised_df = pd.read_csv("/content/denoised_signal_nail_biting_Person1.csv")

print(len(original_df))
print(len(denoised_df))

# Copy timestamp column
denoised_df["Timestamp"] = original_df["Timestamp"].values

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_nail_biting_withTime_Person1.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("denoised_signal_for_nail_biting_withTime_Person1.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/nail_biting/person2_nail.xlsx")

# Save as CSV
df.to_csv("/content/labelled_nail_biting_Person1.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_nail_biting_Person1.csv")

# Apply offset (change this to your computed value)
offset = 0.00
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_nail_biting_Person1.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_nail_biting_Person1.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/labeled_nail_biting_Person1.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_nail_biting_Person1.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

print("Columns in features DataFrame:")
print(features.columns.tolist())
print("\nFirst few rows:")
print(features.head())

# Check if columns might be named differently
print(features.columns.tolist())

# Clean column names first (remove trailing space)
features.columns = features.columns.str.strip()

# If start/end are already in seconds, you don't need to divide by fs
# But if they're in sample indices, then:
fs = 50

# Option 1: If start/end are already in seconds (time)
features["window_start_time"] = features["start"]
features["window_end_time"] = features["end"]

# Option 2: If start/end are sample indices that need conversion
features["window_start_time"] = features["start"] / fs
features["window_end_time"] = features["end"] / fs

print(features.head())

offset = 0.00  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_nail_biting_Person1.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_nail_biting.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_nail_biting_Person1.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'nail': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_nail_biting_Person1.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_nail_biting_Person1.csv")
df.head()

"""# Labelling the data for hair pulling Person1


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Hair_pulling_person1.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_hair_Person1.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling_Person1.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_hair_pulling_withTime_Person1.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_hair_pulling_withTime_Person1.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/hair_pulling/person2_hair.xlsx")

# Save as CSV
df.to_csv("/content/labelled_hair_pulling_Person1.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_hair_pulling_Person1.csv")

# Apply offset (change this to your computed value)
offset = 0.26
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_hair__pulling_Person1.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_hair__pulling_Person1.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_hair_pulling_Person1.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_hair_pulling_Person1.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.26  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_hair_pulling_Person1.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_hair_pulling.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_hair_pulling_Person1.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'hair': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_hair_pulling_Person1.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_hair_pulling_Person1.csv")
df.head()

"""# Labelling the data for face itching Person1


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Face_Scratching_person2.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_face_itching_Person1.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_face_itching_Person1.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_face_itching_withTime_Person1.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_face_itching_withTime_Person1.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/face_itching/person2_face.xlsx")

# Save as CSV
df.to_csv("/content/labelled_face_itching_Person1.csv", index=False)

print("✅ File converted and saved as labelled_face_itching.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_face_itching_Person1.csv")

# Apply offset (change this to your computed value)
offset =3.08
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_face_itching_Person1.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_face_itching_Person1.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_face_itching_Person1.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_face_itching_Person1.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 3.08 # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_face_itching_Person1.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_face_itching.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_face_itching_Person1.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'face': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_face_itching_Person1.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_face_itching_Person1.csv")
df.head()

"""# Labelling the data for beard pulling


"""

# Load your dataframes
original_df = pd.read_csv("/content/drive/MyDrive/CARE_B/Archive (20)/Beard_pulling_person1.csv")
denoised_df = pd.read_csv("/content/beard_pulling_filtered_Person1.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling_Person1.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_beard_pulling_withTime_Person1.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_beard_pulling_withTime_Person1.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/drive/MyDrive/CARE_B/beard_pulling/peraon2_beard_.xlsx")

# Save as CSV
df.to_csv("/content/labelled_beard_pulling_Person1.csv", index=False)

print("✅ File converted and saved as labelled_beard_pulling_.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_beard_pulling_Person1.csv")

# Apply offset (change this to your computed value)
offset = 0.78
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_beard_pulling_Person1.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_beard_pulling_Person1.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_beard_pulling_Person1.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_beard_pulling_Person1.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.78  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_beard_pulling_Person1.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_beard_pulling_Person1.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_beard_pulling_Person1.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'beard': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_beard_pulling_Person1.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_beard_pulling_Person1.csv")
df.head()

"""# Labelling the data for nail biting

"""

# Load your dataframes
original_df = pd.read_csv("/content/nail biting.csv")
denoised_df = pd.read_csv("/content/denoised_signal_nail_biting.csv")

print(len(original_df))
print(len(denoised_df))

# Copy timestamp column
denoised_df["Timestamp"] = original_df["Timestamp"].values

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_nail_biting_withTime.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("denoised_signal_for_nail_biting_withTime.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/labelled_nail_biting_excel.xlsx")

# Save as CSV
df.to_csv("/content/labelled_nail_biting.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_nail_biting.csv")

# Apply offset (change this to your computed value)
offset = 0.51
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end"] + offset

ann.to_csv("annotations_aligned_nail_biting.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_nail_biting.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_nail_biting.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_nail_biting.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.51  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end"] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_nail_biting.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_nail_biting.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_nail_biting.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'nail': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_nail_biting.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_nail_biting.csv")
df.head()

"""# Labelling the data for hair pulling


"""

# Load your dataframes
original_df = pd.read_csv("/content/hair pulling.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_hair.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_hair_pulling_withTime.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_hair_pulling_withTime.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/labelled_hair_pulling_excel.xlsx")

# Save as CSV
df.to_csv("/content/labelled_hair_pulling.csv", index=False)

print("✅ File converted and saved as labelled_nail_biting.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_hair_pulling.csv")

# Apply offset (change this to your computed value)
offset = 0.71
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end"] + offset

ann.to_csv("annotations_aligned_for_hair__pulling.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_hair__pulling.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_hair_pulling.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_hair_pulling.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.71  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end"] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_hair_pulling.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_hair_pulling.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_hair_pulling.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'hair': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_hair_pulling.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_hair_pulling.csv")
df.head()

"""# Labelling the data for face itching


"""

# Load your dataframes
original_df = pd.read_csv("/content/itching on face.csv")
denoised_df = pd.read_csv("/content/denoised_signals_filtered_face_itching.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_face_itching.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_face_itching_withTime.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_face_itching_withTime.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/labelled_face_itching_excel.xlsx")

# Save as CSV
df.to_csv("/content/labelled_face_itching.csv", index=False)

print("✅ File converted and saved as labelled_face_itching.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_face_itching.csv")

# Apply offset (change this to your computed value)
offset = 0.51
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_face_itching.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_face_itching.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_face_itching.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_face_itching.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.51  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_face_itching.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_face_itching.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_face_itching.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'face': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_face_itching.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_face_itching.csv")
df.head()

"""# Labelling the data for beard pulling


"""

# Load your dataframes
original_df = pd.read_csv("/content/beard pulling.csv")
denoised_df = pd.read_csv("/content/beard_pulling_filtered.csv")

print("Original length:", len(original_df))
print("Denoised length:", len(denoised_df))

min_len = min(len(original_df), len(denoised_df))
original_df = original_df.iloc[:min_len]
denoised_df = denoised_df.iloc[:min_len]

denoised_df["Timestamp"] = original_df["Timestamp"].values
denoised_df.to_csv("denoised_signal_for_hair_pulling.csv", index=False)

# Reorder columns to put Timestamp first (optional)
cols = ["Timestamp"] + [c for c in denoised_df.columns if c != "Timestamp"]
denoised_df = denoised_df[cols]

# Save the updated denoised file
denoised_df.to_csv("denoised_signal_for_beard_pulling_withTime.csv", index=False)

print("✅ Denoised dataframe updated with original timestamps.")
print(denoised_df.head())

# Load your denoised data with timestamps
df = pd.read_csv("/content/denoised_signal_for_beard_pulling_withTime.csv")

# Convert UNIX timestamp to relative time (seconds from start)
df["Time_sec"] = df["Timestamp"] - df["Timestamp"].iloc[0]

# Choose the most relevant rotation axis (try RotationX first)
signal = df["RotationX"]

# Plot the first few seconds to visually confirm
plt.figure(figsize=(10, 4))
plt.plot(df["Time_sec"], signal, label="RotationX")
plt.xlim(0, 5)
plt.xlabel("Time (s)")
plt.ylabel("RotationX")
plt.title("Head Nod Detection Region (First 5 seconds)")
plt.legend()
plt.show()

# Detect peaks in RotationX
peaks, _ = find_peaks(np.abs(signal), height=np.std(signal)*2)

# Take the first major peak (likely the nod)
nod_start_idx = peaks[0] if len(peaks) > 0 else 0
nod_start_time = df.loc[nod_start_idx, "Time_sec"]

print(f"⏱ Head nod detected around IMU time: {nod_start_time:.2f} seconds")

# Load the Excel file
df = pd.read_excel("/content/labelled_beard_pulling_excel.xlsx")

# Save as CSV
df.to_csv("/content/labelled_beard_pulling.csv", index=False)

print("✅ File converted and saved as labelled_beard_pulling.csv")

# Load annotations
ann = pd.read_csv("/content/labelled_beard_pulling.csv")

# Apply offset (change this to your computed value)
offset = 0.51
ann["imu_start"] = ann["start"] + offset
ann["imu_end"] = ann["end "] + offset

ann.to_csv("annotations_aligned_for_beard_pulling.csv", index=False)
print(ann)

df = pd.read_csv("/content/annotations_aligned_for_beard_pulling.csv")
df.head()

# Features file (the large one)
features = pd.read_csv("/content/final_features_beard_pulling.csv")

# Labeled intervals file (converted from Excel)
ann = pd.read_csv("/content/labelled_beard_pulling.csv")

# Display a quick check
print(features.columns[-10:])  # last few columns
print(ann.head())

ann["activity"] = ann["activity"].str.strip().str.lower()

fs = 50  # change if your IMU frequency is different

features["window_start_time"] = features["start_idx"] / fs
features["window_end_time"]   = features["end_idx"] / fs

offset = 0.51  # adjust to your actual IMU nod time
ann["imu_start"] = ann["start"] + offset
ann["imu_end"]   = ann["end "] + offset

def assign_label(row, annotations):
    for _, ann_row in annotations.iterrows():
        overlap_start = max(row["window_start_time"], ann_row["imu_start"])
        overlap_end = min(row["window_end_time"], ann_row["imu_end"])
        overlap = max(0, overlap_end - overlap_start)
        window_len = row["window_end_time"] - row["window_start_time"]
        if overlap / window_len >= 0.5:
            return ann_row["activity"]
    return "unlabeled"

features["activity"] = features.apply(lambda r: assign_label(r, ann), axis=1)

features.to_csv("/content/final_features_labeled_beard_pulling.csv", index=False)
print("✅ Saved labeled dataset as final_features_labeled_beard_pulling.csv")
print(features["activity"].value_counts())

df = pd.read_csv("/content/final_features_labeled_beard_pulling.csv")
df.head()

# clean text labels
df['activity'] = df['activity'].str.strip().str.lower()

# check unique values
print(df['activity'].unique())

# define mapping again (in lowercase)
label_map = {
    'head nod': 0,
    'idle': 1,
    'false alarm': 2,
    'beard': 3
}

# apply mapping
df['activity_label'] = df['activity'].map(label_map)

# verify again
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

# verify counts
print(df['activity_label'].value_counts())
print(df[['activity', 'activity_label']].drop_duplicates())

df.head()

# Check counts before mapping (text labels)
print("Before mapping (activity):")
print(df["activity"].value_counts())

# Check counts after mapping (numeric labels)
print("\nAfter mapping (activity_label):")
print(df["activity_label"].value_counts())

# Optional: verify one-to-one mapping consistency
mapping_check = df.groupby("activity")["activity_label"].unique()
print("\nMapping consistency check:")
print(mapping_check)

# Assuming df is your final labeled dataframe
output_path = "/content/final_features_labeled_numeric_beard_pulling.csv"
df.to_csv(output_path, index=False)
print(f"Labeled dataset saved successfully to: {output_path}")

df = pd.read_csv("/content/final_features_labeled_numeric_beard_pulling.csv")
df.head()

"""# Adding Enhanced Features for Distinguishing activities more clearly"""

def add_enhanced_features(df):
    """
    Add advanced features that better distinguish between activities

    Key insights:
    - nail_biting: Repetitive, small movements near face
    - hair_pulling: Upward hand movement, longer duration
    - beard_pulling: Lower face, side-to-side movements
    - face_itching: Quick, scratching motions
    - idle: Minimal movement
    - false_alarm: Random, non-repetitive movements
    """

    print("🔧 Adding enhanced features...")

    # ==========================================
    # 1. Movement Direction Features
    # ==========================================
    print("   📐 Computing movement direction features...")

    # Vertical vs horizontal movement ratio
    df['vertical_horizontal_ratio'] = np.abs(df['acc_mean_z']) / (np.abs(df['acc_mean_x']) + np.abs(df['acc_mean_y']) + 1e-6)

    # Upward movement indicator (hair pulling tends to be upward)
    df['upward_movement'] = df['acc_mean_z']

    # Side-to-side movement (beard pulling)
    df['lateral_movement'] = np.sqrt(df['acc_mean_x']**2 + df['acc_mean_y']**2)

    # ==========================================
    # 2. Movement Intensity Features
    # ==========================================
    print("   ⚡ Computing movement intensity features...")

    # Total acceleration magnitude
    df['total_acc_magnitude'] = np.sqrt(df['acc_mean_x']**2 + df['acc_mean_y']**2 + df['acc_mean_z']**2)

    # Total gyro magnitude
    df['total_gyro_magnitude'] = np.sqrt(df['gyro_mean_x']**2 + df['gyro_mean_y']**2 + df['gyro_mean_z']**2)

    # Movement intensity (combined acc + gyro)
    df['movement_intensity'] = df['acc_energy'] + df['gyro_energy']

    # Jerk (rate of change of acceleration - for scratching/itching)
    df['jerk_estimate'] = df['acc_std_mag'] / (df['acc_mean_mag'] + 1e-6)

    # ==========================================
    # 3. Repetition & Periodicity Features
    # ==========================================
    print("   🔁 Computing repetition features...")

    # Repetitiveness score (nail biting is very repetitive)
    if 'acc_spectral_entropy' in df.columns:
        df['repetitiveness'] = 1 / (df['acc_spectral_entropy'] + 1e-6)

    # Dominant frequency strength (strong peak = repetitive)
    if 'acc_dominant_freq' in df.columns and 'gyro_dominant_freq' in df.columns:
        df['dominant_freq_strength'] = df['acc_dominant_freq'] * df['gyro_dominant_freq']

    # Low frequency dominance (slow repetitive movements)
    if 'acc_0_3Hz' in df.columns:
        total_freq_power = df['acc_0_3Hz'] + df['acc_3_6Hz'] + df['acc_6_12Hz'] + 1e-6
        df['low_freq_ratio'] = df['acc_0_3Hz'] / total_freq_power

    # ==========================================
    # 4. Coordination Features
    # ==========================================
    print("   🤝 Computing coordination features...")

    # Acc-Gyro correlation (coordinated movements)
    df['acc_gyro_coordination'] = (df['acc_energy'] * df['gyro_energy']) / (df['acc_mean_mag'] + df['gyro_mean_mag'] + 1e-6)

    # XY plane activity (face-level movements)
    df['xy_plane_activity'] = np.sqrt(df['acc_std_x']**2 + df['acc_std_y']**2)

    # Z-axis dominance (vertical movements)
    df['z_axis_dominance'] = df['acc_std_z'] / (df['acc_std_x'] + df['acc_std_y'] + 1e-6)

    # ==========================================
    # 5. Stability & Smoothness Features
    # ==========================================
    print("   📊 Computing stability features...")

    # Movement smoothness (idle has smooth/minimal movement)
    df['movement_smoothness'] = df['acc_rms'] / (df['acc_std_mag'] + 1e-6)

    # Gyro stability (stable = idle, unstable = active)
    df['gyro_stability'] = df['gyro_mean_mag'] / (df['gyro_std_mag'] + 1e-6)

    # Acceleration variability
    df['acc_variability'] = (df['acc_std_x'] + df['acc_std_y'] + df['acc_std_z']) / 3

    # ==========================================
    # 6. Activity-Specific Indicators
    # ==========================================
    print("   🎯 Computing activity-specific features...")

    # Nail biting indicator: high repetition + moderate intensity
    df['nail_biting_score'] = (
        df.get('repetitiveness', 0) *
        df['movement_intensity'] / (df['movement_intensity'].max() + 1e-6)
    )

    # Hair pulling indicator: upward + high intensity
    df['hair_pulling_score'] = (
        np.maximum(df['upward_movement'], 0) *
        df['total_acc_magnitude']
    )

    # Beard pulling indicator: lateral + moderate intensity
    df['beard_pulling_score'] = (
        df['lateral_movement'] *
        df['xy_plane_activity']
    )

    # Face itching indicator: high jerk + high frequency
    df['face_itching_score'] = (
        df['jerk_estimate'] *
        df.get('acc_3_6Hz', 1)
    )

    # Idle indicator: low movement + high stability
    df['idle_score'] = (
        1 / (df['movement_intensity'] + 1) *
        df.get('gyro_stability', 1)
    )

    # ==========================================
    # 7. Statistical Moments
    # ==========================================
    print("   📈 Computing statistical moment features...")

    # Coefficient of variation
    df['acc_cv'] = df['acc_std_mag'] / (df['acc_mean_mag'] + 1e-6)
    df['gyro_cv'] = df['gyro_std_mag'] / (df['gyro_mean_mag'] + 1e-6)

    # Signal-to-noise ratio
    df['acc_snr'] = df['acc_mean_mag'] / (df['acc_std_mag'] + 1e-6)
    df['gyro_snr'] = df['gyro_mean_mag'] / (df['gyro_std_mag'] + 1e-6)

    # ==========================================
    # 8. Interaction Features
    # ==========================================
    print("   🔗 Computing interaction features...")

    # Key interactions
    df['acc_gyro_ratio'] = df['acc_energy'] / (df['gyro_energy'] + 1e-6)
    df['freq_intensity_interaction'] = df.get('acc_dominant_freq', 1) * df['movement_intensity']
    df['std_mean_ratio'] = (df['acc_std_mag'] + df['gyro_std_mag']) / (df['acc_mean_mag'] + df['gyro_mean_mag'] + 1e-6)

    print(f"✅ Added {len([c for c in df.columns if c not in ['activity', 'activity_label', 'activity_unified']])} features\n")

    return df

if __name__ == "__main__":
    # Load your existing features
    print("="*70)
    print("🚀 ENHANCED FEATURE ENGINEERING")
    print("="*70)

    FEATURE_FILES = {
        'nail_biting': '/content/final_features_labeled_numeric_nail_biting.csv',
        'hair_pulling': '/content/final_features_labeled_numeric_hair_pulling.csv',
        'beard_pulling': '/content/final_features_labeled_numeric_beard_pulling.csv',
        'face_itching': '/content/final_features_labeled_numeric_face_itching.csv'
      }

    all_enhanced_dfs = []

    for activity_name, file_path in FEATURE_FILES.items():
        try:
            print(f"\n📁 Processing: {activity_name}")
            df = pd.read_csv(file_path)
            print(f"   Original features: {len(df.columns)}")

            # Add enhanced features
            df_enhanced = add_enhanced_features(df)

            print(f"   Enhanced features: {len(df_enhanced.columns)}")
            print(f"   New features added: {len(df_enhanced.columns) - len(df.columns)}")

            # Save enhanced features
            output_file = file_path.replace('.csv', '_enhanced.csv')
            df_enhanced.to_csv(output_file, index=False)
            print(f"   ✅ Saved: {output_file}")

            all_enhanced_dfs.append(df_enhanced)

        except Exception as e:
            print(f"   ❌ Error: {e}")

    # Create combined enhanced dataset
    if all_enhanced_dfs:
        combined_enhanced = pd.concat(all_enhanced_dfs, ignore_index=True)
        combined_enhanced.to_csv('/content/features_all_activities_enhanced.csv', index=False)
        print(f"\n✅ Combined enhanced dataset saved: features_all_activities_enhanced.csv")
        print(f"   Total samples: {len(combined_enhanced)}")
        print(f"   Total features: {len(combined_enhanced.columns)}")

"""# Final Model Improved"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif
import xgboost as xgb
import lightgbm as lgb
from scipy.stats import mode
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("🎯 STRATEGIC ML ACCURACY BOOST")
print("   Focus: Deep Ensemble + Hyperparameter Tuning + Feature Engineering")
print("="*70)

FEATURE_FILES = {
    'nail_biting': [
        '/content/final_features_labeled_numeric_nail_biting_Ananya.csv',
        '/content/final_features_labeled_numeric_nail_biting_Person1.csv',
        '/content/final_features_labeled_numeric_nail_biting_Person2.csv',
    ],
    'beard_pulling': [
        '/content/final_features_labeled_numeric_beard_pulling_Person1.csv',
        '/content/final_features_labeled_numeric_beard_pulling_Person2.csv',
    ],
    'face_itching': [
        '/content/final_features_labeled_numeric_face_itching_Ananya.csv',
        '/content/final_features_labeled_numeric_face_itching_Person1.csv',
        '/content/final_features_labeled_numeric_face_itching_Person2.csv',
    ],
    'hair_pulling': [
        '/content/final_features_labeled_numeric_hair_pulling_Ananya.csv',
        '/content/final_features_labeled_numeric_hair_pulling_Person1.csv',
        '/content/final_features_labeled_numeric_hair_pulling_Person2.csv',
    ]
}

ACTIVITY_MAPPING = {
    'nail': 'nail_biting',
    'hair': 'hair_pulling',
    'beard': 'beard_pulling',
    'face': 'face_itching',
    'itch': 'face_itching',
    'head nod': 'head_nod',
    'idle': 'idle',
    'false alarm': 'false_alarm'
}

print("\n" + "="*70)
print("📁 LOADING DATA")
print("="*70)

all_data = []
for activity_name, file_paths in FEATURE_FILES.items():
    for idx, file_path in enumerate(file_paths, 1):
        try:
            df = pd.read_csv(file_path)
            all_data.append(df)
        except Exception as e:
            print(f"   ✗ Error loading {file_path}: {e}")

combined_df = pd.concat(all_data, ignore_index=True)
print(f"✅ Loaded: {combined_df.shape}")

print("\n" + "="*70)
print("🔧 PREPROCESSING")
print("="*70)

# Label mapping
if 'activity' in combined_df.columns:
    combined_df['activity'] = combined_df['activity'].astype(str).str.strip().str.lower()
else:
    activity_cols = [col for col in combined_df.columns if 'activity' in col.lower() or 'label' in col.lower()]
    if activity_cols:
        combined_df['activity'] = combined_df[activity_cols[0]]

def map_activity(activity):
    activity = str(activity).strip().lower()
    for key, value in ACTIVITY_MAPPING.items():
        if key in activity:
            return value
    return activity

combined_df['activity_unified'] = combined_df['activity'].apply(map_activity)

drop_patterns = [
    'window_start_time', 'window_end_time', 'start_idx', 'end_idx', 'start', 'end',
    'window_id', 'index', 'Unnamed', 'activity', 'activity_label',
    'source_activity', 'source_file', '_fft', 'fft_freqs'
]

drop_cols = []
for pattern in drop_patterns:
    drop_cols.extend([col for col in combined_df.columns if pattern in col.lower()])
drop_cols = list(set(drop_cols))
if 'activity_unified' in drop_cols:
    drop_cols.remove('activity_unified')

combined_df = combined_df.drop(columns=[col for col in drop_cols if col in combined_df.columns])

feature_cols = [col for col in combined_df.columns if col != 'activity_unified']
X = combined_df[feature_cols].copy()
y = combined_df['activity_unified'].copy()

# Convert to numeric
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.median())

# Remove constant and near-constant features
feature_variance = X.var()
low_variance_cols = feature_variance[feature_variance < 0.01].index.tolist()
if low_variance_cols:
    print(f"   Removing {len(low_variance_cols)} low-variance features")
    X = X.drop(columns=low_variance_cols)

MIN_SAMPLES = 30
value_counts = y.value_counts()
valid_classes = value_counts[value_counts >= MIN_SAMPLES].index
if len(valid_classes) < len(value_counts):
    mask = y.isin(valid_classes)
    X = X[mask].reset_index(drop=True)
    y = y[mask].reset_index(drop=True)

print(f"\n📊 Class distribution:")
for activity, count in y.value_counts().items():
    print(f"   {activity:<25} {count:>4} ({count/len(y)*100:>5.1f}%)")

print("\n" + "="*70)
print("🎨 FEATURE ENGINEERING")
print("="*70)

# 1. Statistical aggregations
print("\n📊 Creating statistical features...")
feature_groups = {
    'accel': [col for col in X.columns if 'accel' in col.lower()],
    'gyro': [col for col in X.columns if 'gyro' in col.lower() or 'rotation' in col.lower()],
}

# Overall statistics
for group_name, group_cols in feature_groups.items():
    if len(group_cols) >= 3:
        group_data = X[group_cols[:3]]  # x, y, z

        # Magnitude
        X[f'{group_name}_magnitude'] = np.sqrt((group_data**2).sum(axis=1))

        # Angular features
        X[f'{group_name}_angle_xy'] = np.arctan2(group_data.iloc[:, 1], group_data.iloc[:, 0])
        X[f'{group_name}_angle_xz'] = np.arctan2(group_data.iloc[:, 2], group_data.iloc[:, 0])

        # Statistical measures
        X[f'{group_name}_range'] = group_data.max(axis=1) - group_data.min(axis=1)
        X[f'{group_name}_cv'] = group_data.std(axis=1) / (group_data.mean(axis=1).abs() + 1e-10)

print("   Creating ratio features...")
mean_features = [col for col in X.columns if 'mean' in col.lower()]
std_features = [col for col in X.columns if 'std' in col.lower()]

for mean_col, std_col in zip(mean_features[:6], std_features[:6]):
    if mean_col in X.columns and std_col in X.columns:
        feature_name = mean_col.replace('mean', 'snr')
        X[feature_name] = X[mean_col] / (X[std_col] + 1e-10)

# 3. Cross-sensor correlations
print("   Creating cross-sensor features...")
accel_mean_cols = [col for col in X.columns if 'accel' in col.lower() and 'mean' in col.lower()][:3]
gyro_mean_cols = [col for col in X.columns if 'gyro' in col.lower() and 'mean' in col.lower()][:3]

if len(accel_mean_cols) == 3 and len(gyro_mean_cols) == 3:
    for i in range(3):
        X[f'accel_gyro_product_{i}'] = X[accel_mean_cols[i]] * X[gyro_mean_cols[i]]

print(f"✅ Total features: {X.shape[1]} (added {X.shape[1] - len(feature_cols)})")

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"\n🔀 Split: Train={len(X_train)}, Test={len(X_test)}, Classes={len(le.classes_)}")

print("\n" + "="*70)
print("🎯 INTELLIGENT FEATURE SELECTION")
print("="*70)

# Scale first
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Method 1: Mutual Information (for non-linear relationships)
print("\n📊 Computing mutual information...")
mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=42)
mi_ranking = pd.DataFrame({
    'feature': X.columns,
    'mi_score': mi_scores
}).sort_values('mi_score', ascending=False)

# Method 2: F-statistic (for linear relationships)
print("📊 Computing F-scores...")
f_scores, _ = f_classif(X_train_scaled, y_train)
f_ranking = pd.DataFrame({
    'feature': X.columns,
    'f_score': f_scores
}).sort_values('f_score', ascending=False)

# Combine rankings (ensemble feature selection)
mi_ranking['mi_rank'] = range(len(mi_ranking))
f_ranking['f_rank'] = range(len(f_ranking))
combined_ranking = pd.merge(mi_ranking, f_ranking[['feature', 'f_rank']], on='feature')
combined_ranking['avg_rank'] = (combined_ranking['mi_rank'] + combined_ranking['f_rank']) / 2
combined_ranking = combined_ranking.sort_values('avg_rank')

# Select top features
n_features_to_select = min(100, len(X.columns))  # Top 100 or all if less
selected_features = combined_ranking.head(n_features_to_select)['feature'].tolist()

print(f"\n🔝 Top 20 Features:")
print(combined_ranking.head(20)[['feature', 'mi_score', 'avg_rank']].to_string(index=False))

feature_indices = [list(X.columns).index(f) for f in selected_features]
X_train_selected = X_train_scaled[:, feature_indices]
X_test_selected = X_test_scaled[:, feature_indices]

print(f"\n✅ Selected {len(selected_features)} features")

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}
sample_weights = np.array([class_weight_dict[y] for y in y_train])

print("\n" + "="*70)
print("🔧 TRAINING HYPERPARAMETER-TUNED MODELS")
print("="*70)

# Model 1: Tuned Random Forest
print("\n🌲 Random Forest with GridSearch...")
rf_param_grid = {
    'n_estimators': [300, 500],
    'max_depth': [20, 30, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),
    rf_param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)
rf_grid.fit(X_train_selected, y_train)
rf_best = rf_grid.best_estimator_

print(f"   Best params: {rf_grid.best_params_}")
print(f"   CV Score: {rf_grid.best_score_:.4f}")

rf_pred = rf_best.predict(X_test_selected)
rf_acc = accuracy_score(y_test, rf_pred)
rf_f1 = f1_score(y_test, rf_pred, average='weighted')
print(f"   Test Accuracy: {rf_acc:.4f} | F1: {rf_f1:.4f}")

# --- Model 1: Tuned Random Forest ---
print("\n🌲 Random Forest with GridSearch (Optimizing F1-Weighted)...")
# rf_param_grid remains the same

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),
    rf_param_grid,
    cv=3,
    # 🌟 CHANGE 1: Use 'f1_weighted' scoring
    scoring='f1_weighted',
    n_jobs=-1,
    verbose=0
)
rf_grid.fit(X_train_selected, y_train)
rf_best = rf_grid.best_estimator_

print("\n🚀 XGBoost with GridSearch...")
xgb_param_grid = {
    'n_estimators': [300, 500],
    'max_depth': [6, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_grid = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False, n_jobs=-1),
    xgb_param_grid,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)
xgb_grid.fit(X_train_selected, y_train, sample_weight=sample_weights)
xgb_best = xgb_grid.best_estimator_

print(f"   Best params: {xgb_grid.best_params_}")
print(f"   CV Score: {xgb_grid.best_score_:.4f}")

xgb_pred = xgb_best.predict(X_test_selected)
xgb_acc = accuracy_score(y_test, xgb_pred)
xgb_f1 = f1_score(y_test, xgb_pred, average='weighted')
print(f"   Test Accuracy: {xgb_acc:.4f} | F1: {xgb_f1:.4f}")

print("\n🚀 XGBoost with GridSearch (Optimizing F1-Weighted)...")
# xgb_param_grid remains the same

xgb_grid = GridSearchCV(
    xgb.XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False, n_jobs=-1),
    xgb_param_grid,
    cv=3,
    # 🌟 CHANGE 2: Use 'f1_weighted' scoring
    scoring='f1_weighted',
    n_jobs=-1,
    verbose=0
)
# Note: sample_weight is still used for fit, but optimization is F1
xgb_grid.fit(X_train_selected, y_train, sample_weight=sample_weights)
xgb_best = xgb_grid.best_estimator_

print("\n💡 LightGBM...")
lgb_model = lgb.LGBMClassifier(
    n_estimators=500,
    max_depth=15,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbose=-1
)
lgb_model.fit(X_train_selected, y_train)
lgb_pred = lgb_model.predict(X_test_selected)
lgb_acc = accuracy_score(y_test, lgb_pred)
lgb_f1 = f1_score(y_test, lgb_pred, average='weighted')
print(f"   Test Accuracy: {lgb_acc:.4f} | F1: {lgb_f1:.4f}")

print("\n📈 Gradient Boosting...")
gb_model = GradientBoostingClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.05,
    subsample=0.8,
    min_samples_split=5,
    random_state=42
)
gb_model.fit(X_train_selected, y_train, sample_weight=sample_weights)
gb_pred = gb_model.predict(X_test_selected)
gb_acc = accuracy_score(y_test, gb_pred)
gb_f1 = f1_score(y_test, gb_pred, average='weighted')
print(f"   Test Accuracy: {gb_acc:.4f} | F1: {gb_f1:.4f}")

print("\n🌳 Extra Trees...")
et_model = ExtraTreesClassifier(
    n_estimators=500,
    max_depth=30,
    min_samples_split=2,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
et_model.fit(X_train_selected, y_train)
et_pred = et_model.predict(X_test_selected)
et_acc = accuracy_score(y_test, et_pred)
et_f1 = f1_score(y_test, et_pred, average='weighted')
print(f"   Test Accuracy: {et_acc:.4f} | F1: {et_f1:.4f}")

print("\n" + "="*70)
print("🎭 ADVANCED ENSEMBLE STRATEGIES")
print("="*70)

# Get all predictions and probabilities
all_preds = np.array([rf_pred, xgb_pred, lgb_pred, gb_pred, et_pred])
all_probas = np.array([
    rf_best.predict_proba(X_test_selected),
    xgb_best.predict_proba(X_test_selected),
    lgb_model.predict_proba(X_test_selected),
    gb_model.predict_proba(X_test_selected),
    et_model.predict_proba(X_test_selected)
])

# Strategy 1: Simple Majority Voting
print("\n🗳️  Majority Voting...")
majority_pred, _ = mode(all_preds, axis=0)
majority_pred = majority_pred.ravel()
majority_acc = accuracy_score(y_test, majority_pred)
majority_f1 = f1_score(y_test, majority_pred, average='weighted')
print(f"   Accuracy: {majority_acc:.4f} | F1: {majority_f1:.4f}")

# Strategy 2: Weighted Voting (by accuracy)
print("\n⚖️  Weighted Voting...")
weights = np.array([rf_acc, xgb_acc, lgb_acc, gb_acc, et_acc])
weights = weights / weights.sum()
weighted_proba = np.average(all_probas, axis=0, weights=weights)
weighted_pred = np.argmax(weighted_proba, axis=1)
weighted_acc = accuracy_score(y_test, weighted_pred)
weighted_f1 = f1_score(y_test, weighted_pred, average='weighted')
print(f"   Weights: {weights}")
print(f"   Accuracy: {weighted_acc:.4f} | F1: {weighted_f1:.4f}")

print("\n📚 Rank-Based Weighted Voting...")
# Identify top 3 models
accuracies = [rf_acc, xgb_acc, lgb_acc, gb_acc, et_acc]
top3_indices = np.argsort(accuracies)[-3:]
top3_weights = np.array([accuracies[i] for i in top3_indices])
top3_weights = top3_weights / top3_weights.sum()

top3_probas = all_probas[top3_indices]
top3_proba = np.average(top3_probas, axis=0, weights=top3_weights)
top3_pred = np.argmax(top3_proba, axis=1)
top3_acc = accuracy_score(y_test, top3_pred)
top3_f1 = f1_score(y_test, top3_pred, average='weighted')
print(f"   Top 3 models: {[['RF', 'XGB', 'LGB', 'GB', 'ET'][i] for i in top3_indices]}")
print(f"   Accuracy: {top3_acc:.4f} | F1: {top3_f1:.4f}")

print("\n🎯 Confidence-Based Voting...")
# Use prediction with highest confidence
max_confidences = np.max(all_probas, axis=2)
best_model_per_sample = np.argmax(max_confidences, axis=0)
confidence_pred = np.array([all_preds[best_model_per_sample[i], i] for i in range(len(y_test))])
confidence_acc = accuracy_score(y_test, confidence_pred)
confidence_f1 = f1_score(y_test, confidence_pred, average='weighted')
print(f"   Accuracy: {confidence_acc:.4f} | F1: {confidence_f1:.4f}")

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ==============================
# CALCULATE ALL METRICS FOR ALL MODELS
# ==============================

print("\n" + "="*70)
print("📊 COMPLETE METRICS FOR ALL MODELS")
print("="*70)

# Calculate metrics for all models
models_metrics = {
    'Random Forest': rf_pred,
    'XGBoost': xgb_pred,
    'LightGBM': lgb_pred,
    'GradientBoost': gb_pred,
    'ExtraTrees': et_pred,
    'Majority Vote': majority_pred,
    'Weighted Vote': weighted_pred,
    'Top3 Vote': top3_pred,
    'Confidence Vote': confidence_pred
}

# Store all metrics
all_metrics = []

for model_name, predictions in models_metrics.items():
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted', zero_division=0)
    recall = recall_score(y_test, predictions, average='weighted', zero_division=0)
    f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)

    all_metrics.append({
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    })

# Create comprehensive results dataframe
complete_results = pd.DataFrame(all_metrics).sort_values('Accuracy', ascending=False)

# ==============================
# DISPLAY RESULTS
# ==============================

print("\n📋 COMPLETE PERFORMANCE METRICS (Sorted by Accuracy):")
print("="*100)
print(f"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
print("-"*100)

for idx, row in complete_results.iterrows():
    print(f"{row['Model']:<20} {row['Accuracy']:<12.4f} {row['Precision']:<12.4f} "
          f"{row['Recall']:<12.4f} {row['F1-Score']:<12.4f}")

print("="*100)

# ==============================
# BEST MODEL IDENTIFICATION
# ==============================

print("\n" + "="*70)
print("🏆 BEST PERFORMING MODELS")
print("="*70)

best_accuracy = complete_results.iloc[0]
best_precision = complete_results.nlargest(1, 'Precision').iloc[0]
best_recall = complete_results.nlargest(1, 'Recall').iloc[0]
best_f1 = complete_results.nlargest(1, 'F1-Score').iloc[0]

print(f"\n🎯 Best Accuracy:  {best_accuracy['Model']:<20} → {best_accuracy['Accuracy']:.4f}")
print(f"🎯 Best Precision: {best_precision['Model']:<20} → {best_precision['Precision']:.4f}")
print(f"🎯 Best Recall:    {best_recall['Model']:<20} → {best_recall['Recall']:.4f}")
print(f"🎯 Best F1-Score:  {best_f1['Model']:<20} → {best_f1['F1-Score']:.4f}")

# ==============================
# MODEL CATEGORY COMPARISON
# ==============================

print("\n" + "="*70)
print("📊 CATEGORY-WISE PERFORMANCE")
print("="*70)

# Individual models
individual_models = ['Random Forest', 'XGBoost', 'LightGBM', 'GradientBoost', 'ExtraTrees']
individual_metrics = complete_results[complete_results['Model'].isin(individual_models)]

print("\n🌲 INDIVIDUAL MODELS:")
print(f"   Average Accuracy:  {individual_metrics['Accuracy'].mean():.4f}")
print(f"   Average Precision: {individual_metrics['Precision'].mean():.4f}")
print(f"   Average Recall:    {individual_metrics['Recall'].mean():.4f}")
print(f"   Average F1-Score:  {individual_metrics['F1-Score'].mean():.4f}")

# Ensemble models
ensemble_models = ['Majority Vote', 'Weighted Vote', 'Top3 Vote', 'Confidence Vote']
ensemble_metrics = complete_results[complete_results['Model'].isin(ensemble_models)]

print("\n🤝 ENSEMBLE MODELS:")
print(f"   Average Accuracy:  {ensemble_metrics['Accuracy'].mean():.4f}")
print(f"   Average Precision: {ensemble_metrics['Precision'].mean():.4f}")
print(f"   Average Recall:    {ensemble_metrics['Recall'].mean():.4f}")
print(f"   Average F1-Score:  {ensemble_metrics['F1-Score'].mean():.4f}")

improvement = ensemble_metrics['Accuracy'].mean() - individual_metrics['Accuracy'].mean()
print(f"\n📈 Ensemble Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)")

# ==============================
# VISUALIZATION 1: COMPREHENSIVE METRICS BAR CHART
# ==============================

print("\n📊 Generating Comprehensive Metrics Visualization...")

fig, axes = plt.subplots(2, 2, figsize=(20, 16))

metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ['#2E86C1', '#E67E22', '#27AE60', '#8E44AD']

for idx, (ax, metric, color) in enumerate(zip(axes.flat, metrics_to_plot, colors)):
    # Sort by current metric
    sorted_data = complete_results.sort_values(metric, ascending=False)

    # Create bars
    bars = ax.barh(range(len(sorted_data)), sorted_data[metric],
                   color=color, edgecolor='black', linewidth=1.5, alpha=0.8)

    # Highlight best model
    bars[0].set_color(color)
    bars[0].set_alpha(1.0)
    bars[0].set_linewidth(2.5)

    # Add value labels
    for i, (bar, value) in enumerate(zip(bars, sorted_data[metric])):
        ax.text(value + 0.005, i, f'{value:.4f}',
                va='center', fontsize=11, fontweight='bold')

    # Customize
    ax.set_yticks(range(len(sorted_data)))
    ax.set_yticklabels(sorted_data['Model'], fontsize=12, fontweight='bold')
    ax.set_xlabel(metric, fontsize=14, fontweight='bold')
    ax.set_title(f'{metric} Comparison', fontsize=16, fontweight='black', pad=15)
    ax.set_xlim([0, 1.05])
    ax.grid(axis='x', alpha=0.3, linestyle='--', linewidth=1)
    ax.axvline(x=sorted_data[metric].mean(), color='red', linestyle='--',
               linewidth=2, label=f'Average: {sorted_data[metric].mean():.4f}')
    ax.legend(fontsize=10, loc='lower right')

plt.suptitle('Complete Performance Metrics for All Models',
             fontsize=24, fontweight='black', y=0.995)
plt.tight_layout()
plt.savefig('all_models_complete_metrics.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('all_models_complete_metrics.pdf', bbox_inches='tight', facecolor='white', format='pdf')
print("✅ Saved: all_models_complete_metrics.png/pdf")
plt.show()
plt.close()

# ==============================
# VISUALIZATION 2: GROUPED BAR CHART
# ==============================

print("\n📊 Generating Grouped Bar Chart...")

fig, ax = plt.subplots(figsize=(18, 10))

x = np.arange(len(complete_results))
width = 0.2

bars1 = ax.bar(x - 1.5*width, complete_results['Accuracy'], width,
               label='Accuracy', color='#2E86C1', edgecolor='black', linewidth=1.5)
bars2 = ax.bar(x - 0.5*width, complete_results['Precision'], width,
               label='Precision', color='#E67E22', edgecolor='black', linewidth=1.5)
bars3 = ax.bar(x + 0.5*width, complete_results['Recall'], width,
               label='Recall', color='#27AE60', edgecolor='black', linewidth=1.5)
bars4 = ax.bar(x + 1.5*width, complete_results['F1-Score'], width,
               label='F1-Score', color='#8E44AD', edgecolor='black', linewidth=1.5)

# Customize
ax.set_xlabel('Model', fontsize=16, fontweight='bold', labelpad=15)
ax.set_ylabel('Score', fontsize=16, fontweight='bold', labelpad=15)
ax.set_title('All Metrics Comparison Across Models', fontsize=22, fontweight='black', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(complete_results['Model'], rotation=45, ha='right', fontsize=12, fontweight='bold')
ax.legend(fontsize=14, loc='lower right', frameon=True, shadow=True)
ax.set_ylim([0, 1.1])
ax.grid(axis='y', alpha=0.3, linestyle='--', linewidth=1)
ax.tick_params(axis='y', labelsize=12)

# Add average line
avg_line = complete_results[['Accuracy', 'Precision', 'Recall', 'F1-Score']].mean().mean()
ax.axhline(y=avg_line, color='red', linestyle='--', linewidth=2, alpha=0.7,
           label=f'Overall Average: {avg_line:.4f}')

plt.tight_layout()
plt.savefig('all_models_grouped_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('all_models_grouped_comparison.pdf', bbox_inches='tight', facecolor='white', format='pdf')
print("✅ Saved: all_models_grouped_comparison.png/pdf")
plt.show()
plt.close()

# ==============================
# VISUALIZATION 3: HEATMAP
# ==============================

print("\n📊 Generating Metrics Heatmap...")

fig, ax = plt.subplots(figsize=(14, 10))

# Prepare data for heatmap
heatmap_data = complete_results[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values

# Create heatmap
im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)

# Set ticks
ax.set_xticks(np.arange(4))
ax.set_yticks(np.arange(len(complete_results)))
ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'],
                   fontsize=14, fontweight='bold')
ax.set_yticklabels(complete_results['Model'], fontsize=12, fontweight='bold')

# Add colorbar
cbar = plt.colorbar(im, ax=ax, pad=0.02)
cbar.ax.tick_params(labelsize=12)
cbar.set_label('Score', fontsize=14, fontweight='bold', labelpad=15)

# Add text annotations
for i in range(len(complete_results)):
    for j in range(4):
        text = ax.text(j, i, f'{heatmap_data[i, j]:.3f}',
                      ha="center", va="center", color="black",
                      fontsize=11, fontweight='bold')

# Title
ax.set_title('Performance Heatmap: All Metrics Across All Models',
             fontsize=20, fontweight='black', pad=20)

plt.tight_layout()
plt.savefig('all_models_heatmap.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('all_models_heatmap.pdf', bbox_inches='tight', facecolor='white', format='pdf')
print("✅ Saved: all_models_heatmap.png/pdf")
plt.show()
plt.close()

# ==============================
# VISUALIZATION 4: RADAR CHART (TOP 5 MODELS)
# ==============================

print("\n📊 Generating Radar Chart for Top 5 Models...")

from math import pi

# Select top 5 models by accuracy
top5_models = complete_results.head(5)

# Prepare data
categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
N = len(categories)

# Create figure
fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))

# Compute angle for each axis
angles = [n / float(N) * 2 * pi for n in range(N)]
angles += angles[:1]

# Plot for each model
colors_radar = ['#2E86C1', '#E67E22', '#27AE60', '#8E44AD', '#E74C3C']

for idx, (_, row) in enumerate(top5_models.iterrows()):
    values = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]
    values += values[:1]

    ax.plot(angles, values, 'o-', linewidth=3, label=row['Model'],
            color=colors_radar[idx], markersize=8)
    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])

# Customize
ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories, fontsize=14, fontweight='bold')
ax.set_ylim(0, 1)
ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)
ax.grid(True, linestyle='--', alpha=0.5)

# Title and legend
plt.title('Top 5 Models Performance Comparison (Radar Chart)',
          fontsize=20, fontweight='black', pad=30, y=1.08)
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12, frameon=True, shadow=True)

plt.tight_layout()
plt.savefig('top5_models_radar_chart.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('top5_models_radar_chart.pdf', bbox_inches='tight', facecolor='white', format='pdf')
print("✅ Saved: top5_models_radar_chart.png/pdf")
plt.show()
plt.close()

# ==============================
# VISUALIZATION 5: COMPREHENSIVE SUMMARY TABLE
# ==============================

print("\n📊 Generating Comprehensive Summary Table...")

fig, ax = plt.subplots(figsize=(18, 12))
ax.axis('off')

# Prepare table data
table_data = []
table_data.append(['Rank', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Avg Score'])

for idx, (_, row) in enumerate(complete_results.iterrows(), 1):
    avg_score = (row['Accuracy'] + row['Precision'] + row['Recall'] + row['F1-Score']) / 4
    table_data.append([
        f'{idx}',
        row['Model'],
        f'{row["Accuracy"]:.4f}',
        f'{row["Precision"]:.4f}',
        f'{row["Recall"]:.4f}',
        f'{row["F1-Score"]:.4f}',
        f'{avg_score:.4f}'
    ])

# Add average row
avg_acc = complete_results['Accuracy'].mean()
avg_prec = complete_results['Precision'].mean()
avg_rec = complete_results['Recall'].mean()
avg_f1 = complete_results['F1-Score'].mean()
overall_avg = (avg_acc + avg_prec + avg_rec + avg_f1) / 4

table_data.append(['', '', '', '', '', '', ''])
table_data.append([
    '-',
    'AVERAGE',
    f'{avg_acc:.4f}',
    f'{avg_prec:.4f}',
    f'{avg_rec:.4f}',
    f'{avg_f1:.4f}',
    f'{overall_avg:.4f}'
])

# Create table
table = ax.table(cellText=table_data,
                cellLoc='center',
                loc='center',
                bbox=[0, 0, 1, 1])

table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1, 2.2)

# Style cells
header_color = '#2E86C1'
row_colors = ['#E8F4F8', '#FFFFFF']
top3_color = '#FFFACD'  # Light yellow for top 3
avg_color = '#F9E79F'

for i, row in enumerate(table_data):
    for j in range(len(row)):
        cell = table[(i, j)]

        if i == 0:  # Header
            cell.set_facecolor(header_color)
            cell.set_text_props(weight='black', color='white', size=14)
        elif i == len(table_data) - 1:  # Average row
            cell.set_facecolor(avg_color)
            cell.set_text_props(weight='bold', size=13)
        elif i <= 3:  # Top 3 models
            cell.set_facecolor(top3_color)
            cell.set_text_props(weight='bold', size=12)
        else:
            cell.set_facecolor(row_colors[i % 2])
            cell.set_text_props(size=11)

        # Bold model names
        if j == 1:
            cell.set_text_props(weight='bold', ha='left')

        cell.set_edgecolor('#333333')
        cell.set_linewidth(1.5)

plt.title('Complete Performance Summary - All Models\n(Ranked by Accuracy)',
          fontsize=22, fontweight='black', pad=25)

plt.tight_layout()
plt.savefig('all_models_summary_table.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('all_models_summary_table.pdf', bbox_inches='tight', facecolor='white', format='pdf')
print("✅ Saved: all_models_summary_table.png/pdf")
plt.show()
plt.close()

# ==============================
# FINAL SUMMARY
# ==============================

print("\n" + "="*70)
print("🎉 COMPLETE METRICS ANALYSIS FINISHED!")
print("="*70)

print("\n📁 Generated Files:")
print("   1. all_models_complete_metrics.png/pdf - Individual metric comparisons")
print("   2. all_models_grouped_comparison.png/pdf - All metrics side-by-side")
print("   3. all_models_heatmap.png/pdf - Color-coded performance matrix")
print("   4. top5_models_radar_chart.png/pdf - Radar chart for top 5 models")
print("   5. all_models_summary_table.png/pdf - Comprehensive ranked table")

print("\n📊 Summary Statistics:")
print(f"   Total Models Evaluated: {len(complete_results)}")
print(f"   Best Overall Model: {complete_results.iloc[0]['Model']}")
print(f"   Highest Accuracy: {complete_results['Accuracy'].max():.4f}")
print(f"   Highest Precision: {complete_results['Precision'].max():.4f}")
print(f"   Highest Recall: {complete_results['Recall'].max():.4f}")
print(f"   Highest F1-Score: {complete_results['F1-Score'].max():.4f}")

print("\n💡 Key Insights:")
print(f"   • Individual Models Avg: {individual_metrics['Accuracy'].mean():.4f}")
print(f"   • Ensemble Models Avg: {ensemble_metrics['Accuracy'].mean():.4f}")
print(f"   • Best Strategy: {'Ensemble' if ensemble_metrics['Accuracy'].mean() > individual_metrics['Accuracy'].mean() else 'Individual'}")

print("\n" + "="*70)

# Save results to CSV for easy reference
complete_results.to_csv('all_models_complete_metrics.csv', index=False)
print("✅ Saved: all_models_complete_metrics.csv")
print("\n" + "="*70)

# Per-class analysis
print("\n📊 Per-Class Performance:")
print(f"{'Activity':<25} {'Baseline':<12} {'New':<12} {'Change':<12}")
print("-" * 61)

baseline_results = {
    'beard_pulling': 0.5926,
    'face_itching': 0.8158,
    'false_alarm': 0.1765,
    'hair_pulling': 0.7941,
    'idle': 0.4211,
    'nail_biting': 0.8519
}

for i, activity in enumerate(le.classes_):
    mask = y_test == i
    if np.sum(mask) > 0:
        new_acc = np.mean(final_pred[mask] == y_test[mask])
        baseline = baseline_results.get(activity, 0)
        change = new_acc - baseline
        print(f"{activity:<25} {baseline:<12.4f} {new_acc:<12.4f} {change:+.4f}")

# Detailed report
print("\n" + "="*70)
print(f"📋 CLASSIFICATION REPORT - {best_result['Model']}")
print("="*70)
print(classification_report(y_test, final_pred, target_names=le.classes_, zero_division=0))

print("\n📊 Generating visualizations...")

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, final_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f'Confusion Matrix - {best_result["Model"]}\nAccuracy: {best_result["Accuracy"]:.4f}',
          fontsize=14, fontweight='bold')
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('True', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('final_confusion_matrix.png', dpi=300, bbox_inches='tight')
print("✅ Saved: final_confusion_matrix.png")
plt.show()

# Model comparison
fig, ax = plt.subplots(figsize=(14, 6))
colors = ['green' if i == 0 else 'skyblue' for i in range(len(results))]
bars = ax.barh(range(len(results)), results['Accuracy'], color=colors, alpha=0.7)
ax.set_yticks(range(len(results)))
ax.set_yticklabels(results['Model'])
ax.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
ax.axvline(x=0.642, color='red', linestyle='--', linewidth=2, label='Previous Best (64.2%)')
ax.grid(True, alpha=0.3, axis='x')
ax.legend()

for i, (acc, f1) in enumerate(zip(results['Accuracy'], results['F1-Score'])):
    ax.text(acc + 0.005, i, f'{acc:.3f}', va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('final_model_comparison.png', dpi=300, bbox_inches='tight')
print("✅ Saved: final_model_comparison.png")
plt.show()

